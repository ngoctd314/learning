+ Anh có nói: "Từ đặc điểm của bài toán trên, cần chọn một công nghệ lưu trữ và query có những đặc tính sau:"
Em thấy không chính xác ở quan điểm này, theo e thì phải là cần tìm ra cấu trúc dữ liệu phù hợp, khi tìm được cấu trúc dữ liệu thì mới xem xét là có thể tự implement không, nếu khó implement thì tìm xem có công cụ nào implement sẵn cho không.

+ Bài toán khi sử dụng một db mới không phải là nó có ngon thật hay không mà em nghĩ nó là bài toán triển khai, nguồn lực, nó chạy ngon thì không sao, thế nhỡ một ngày nó lăn ra chết thì sao. Nguồn lực của công ty có nhiều người nắm rõ công cụ không, có expert về nó không, nhỡ người nắm rõ công cụ lại có định hướng khác, nghỉ việc thì sao, ai sẽ là người thay thế.
Chứ cứ cái nào benchmark ngon hơn thì dùng, thế thì lựa chọn công nghệ dễ quá cần gì phải "lựa chọn".

+ Giải pháp của anh là dùng clickhouse, còn giải pháp của e là dùng cấu trúc dữ liệu phù hợp, không phụ thuộc db.
Trong giải pháp của e có 1 phần bottleneck là việc lấy data từ db về (network cost), có thể bỏ qua phần này bằng việc sử dụng db có support sẵn bitmap, clickhouse cũng support sẵn, nếu a dùng bitmap thì e dự đoán đâu đó thời gian xử lý nhanh hơn nhiều so với cách lưu 500 tỷ rows và count distinct.

1. Anh có ước lượng: "Nếu mỗi item lưu vào một só 4 byte, thì tổng cổng để lưu tất cả data cần ít nhất: 100000000 * 5000 * 4 / (1024 * 1024 * 1024) = 1862.64 GB"
Trong khi đó lại lưu dạng <id, item> (5000*100M = 500 tỷ rows như này), thì disk phải là: 100000000 * 5000 * (4+4) / (1024 * 1024 * 1024) = 3725.28 GB chứ, lưu id cũng tốn disk mà a, sao lại tính disk cho item thôi.

2. Anh có demo với dữ liệu 5000*5000 (theo đầu bài phải là 10000*5000) trên 100M, với output ra khoảng 600000
**Tức là:** lấy ngẫu nhiên 25_000_000 số (cho phép lấy trùng) trong khoảng [0, 100_000_000) lưu vào tập A, sau đó lấy distinct thì được khoảng 600000 số
**Rút gọn:** lấy ngẫu nhiên 250 số (cho phép lấy trùng) trong khoảng [0, 1_000) lưu vào tập A, sau đó lấy distinct thì được khoảng 6 số
Lượng distinct này có vẻ hơi thấp, anh giải thích cho e với. 

3. Trường hợp unlink thì làm như nào anh, unlink là trường hợp 1 item không còn liên quan đến 1 item:
Anh lưu dạng <id, item>, ví dụ
id, item
1   2
1   3
1   4
1   5
...
1   5000
Khi item 1 không còn liên quan item 2 thì xử lý như nào

4. Trường hợp delete item thì làm như nào anh, delete là trường hợp 1 item bị xóa khỏi kho:
Ví dụ:
1: {2,3,4}
2: {3,4}
3: {1}
4: {1,2}
5: {3,2}

Xóa item 2 khỏi hệ thống thì làm như nào

5. Anh có nói nén mà lại lưu <id,item> (uint32, uint32), nén ở đây làn nén cái gì, nén như nào và giải nén như nào a.

6. Khi đưa ra thời gian chạy cho một chương trình gì đó, nên gắn liền với thông tin servers: dùng mấy con server, mỗi con thì cần có thông tin RAM, CPU (ít nhất gồm CPU family, Thread(s) per core, Core(s) per socket, CPU max MHz). Trừ khi chương trình không phụ thuộc nhiều vào RAM và CPU. A cho e xin cấu hình đó với nhé.

7. Theo a để triển khai giải pháp của a thì cần bao nhiêu resource, maintain giải pháp của a cần bao nhiêu resource. Các team khác gặp cùng vấn đề thì họ áp dụng được một cách dễ dàng hay không.

8. Nói về vấn đề luật chơi, a có tuân thủ không, đầu bài thì đọc thiếu, query thì sai, có người chỉ ra chỗ sai thì a bảo "chỉ là query thôi mà", chỉ là query thôi thì sao a không làm cho đúng, rồi giải pháp là gửi qua mail, a show lên trên này làm gì =)), a sợ ban giám khảo không đọc được giải pháp của anh à.

Người ta làm thì anh bảo "Nhìn là biết sai vấn đề", thế e xin trình bày là: giải pháp thì e trình bày với ban giám khảo 1 buổi rồi, trong buổi đó có phần nén bitmap e chưa hiểu rõ được, nên mới xin thêm thời gian để hiểu rõ cách sử dùng, buổi hôm trước là e trình bày về roaring bitmap chứ không phải trình bày giải pháp.

=> Tóm lại e thấy a đang nói với mọi người là clickhouse query nhanh lắm, dùng clickhouse đi, clickhouse insert + select nhanh thì e đồng ý. Nhưng một solution giải quyết bài toán mà bảo phải dùng cái này, phải dùng cái kia, mà thậm chí còn không giải thích chi tiết được tại sao phải dùng thì e không đồng ý. Vấn đề của bài toán là: union trên tập lớn, không phải là tìm bài toán để sử dụng clickhouse. Công nghệ để ứng dụng vào bài toán, chứ sao lại đi tìm bài toán để ứng dụng công nghệ.

Em nghĩ a nên đọc kĩ đề bài, trước khi chạy với tập dữ liệu lớn thì chạy với tập nhỏ + dùng bruteforce để kiểm thử tính chính xác trước, chứ để mà nói về nhanh thì cout << rand(), nhanh quá sợ người ta nghi ngờ thì sleep lại 1 ít là nhanh như chớp, vô địch thiên hạ.
