# Concurrency Foundations

## 55. Mixing up concurrency and parallelism

Even after years of concurrent programming, developers may not clearly understand the differences between concurrency and parallelism. Before delving into Go-specific topics, it's first essential to understand these concepts so we share a common vocabulary.

Concurrency enables parallelism. Indeed, concurrency provides a structure to solve a problem with parts that may be parallelized.

Concurrency is about dealing with a lot of things at once. Parallelism is about doing lots of things at once.

In summary, concurrency and parallelism are different. Concurrency is about structure, and we can change a sequential implementation into a concurrent one by introduction different steps that separate concurrent threads can tackle. Meanwhile, parallelism is about execution, and we can use it at the step level by adding more parallel threads.

## 56. Thinking concurrency is always faster

A misconception among many developers is believing that a concurrent solution is always faster than a sequential one. This couldn't be more wrong. The overall performance of a solution depends on many factors, such as the efficiency of our structure (concurrency), which parts can be tackled in parallel and the level of contention among the computation units.

### Go scheduling

A thread is the smallest unit of processing that an OS can perform. If a process wants to execute multiple actions simultaneously, it spins up multiple threads. These threads can be
- Concurrent: Two or more threads can start, run and complete in overlapping time periods, like the waiter thread and the coffee machine thread in the previous section
- Parallel: The same task can be executed in multiple times at once, like multiple waiter threads.

The OS is responsible for scheduling the thread's processes optimally so that
- All the threads can consume CPU cycles without being starved for too much time.
- The workload is distributed as evenly as possible among the different CPU cores.

**NOTE** The word thread can also have a different meaning at a CPU level. Each physical core can be composed of multiple logical cores (the concept of hyperthreading), and a logical core is also called a thread. When we use the word thread, we mean the unit of processing, not a logical core. 

A CPU core executes different threads. When it switches from one thread to another, it executes an operation called context switching. The active thread consuming CPU cycles was in an executing state and moves to runnable state, meaning it's ready to be executed pending an available core. Context switching is considered an expensive operation because the OS needs to save the current execution state of a thread before the switch (such as the current register values). 

As Go developers, we can't create threads directly, but we can create goroutines, which can be thought of as application-level threads. However, whereas an OS thread is context-switched on and off a CPU core by the OS, a goroutine is context-switched on and off by the Go runtime. Also, compared to an OS thread, a goroutine has a smaller memory footprint: 2KB for goroutines from Go 1.4. An OS thread depends on the OS, the defautl size is 2MB. Having smaller size makes context switching faster. 

**NOTE** Context switching a goroutine versus a thread is about 80% to 90% faster, depending on the architecture.

Internally, the Go scheduler uses the following terminology

- G - Goroutine
- M - OS thread (stands for machine)
- P - CPU core (stands for processor)

Each OS thread (M) is assigned to a CPU core (P) by the OS scheduler. Then each goroutine (G) runs on an M. The GOMAXPROCS variable defines the limit of Ms in charge of executing user-level code simultaneously. But if a thread is blocked in a system call (I/O), the scheduler can spin up more Ms. As of Go 1.5, GOMAXPROCS is by default equal to the number of available CPU cores.

A goroutine has a simpler lifecycle than an OS thread. It can be doing one of the following:

- Executing - The goroutine is scheduled on an M and executing its instructions.
- Runnable - The goroutine is waiting to be an executing state.
- Waiting - The goroutine is stopped and pending something completing, such as a system call or a synchronization operating (such as acquiring a mutex)

There's one last stage to understand about the implementation of Go scheduling: when a goroutine is created but cannot be executed yet; for example, all the other Ms are already executing a G. In this scenarios, what will the Go runtime do about it? The answer is queuing. The Go runtime handles two kinds of queues: one local queue per P and a global queue shared among all the Ps.

TODO: readmore

### Parallel merge sort

## 57. Being puzzled about when to use channels of mutexes

It may not always be clear whether we can implement a solution using channels or mutexes. Because Go promotes sharing memory by communication, one mistake could be always force the use of channels, regardless of the use case.

First, a brief reminder about channels in Go: channels are a communication mechanism. Internally, a channel is a pipe we can use to send and receive values and that allows us to connect concurrent goroutines. A channel can be either of the following:
- Unbuffed - The sender goroutine blocks until the receiver goroutine is ready.
- Buffered - The sender goroutine blocks only when the buffer is full.

Let's get back to our initial problem. When should we use channels or mutexes? Our example has three different goroutines:

![example](./assets/#57.1.png)

- G1 and G2 are parallel goroutines. They may be two goroutines executing the same function that keeps receiving message from a channel, or perhaps two goroutines executing the same HTTP handler at the same time. 
- On the other hand, G1 and G3 are concurrent goroutines, as are G2 and G3. All the goroutines are part of an overall concurrent structure, but G1 and G3 perform the first step, whereas G3 does the next step.

In general, parallel goroutines have to synchronize: for example, when they need to access or mutate a shared resource such as a slice. Synchronization is enforced with mutexes but not with any channel types (not with buffered channels). Hence, in general, synchronization between parallel goroutines should be achieved via mutexes.

Conversely, in general, concurrent goroutines have to coordinate and orchestrate. For example, if G3 need to aggregate results from both G1 and G2, G1 and G2 need to signal to G3 that a new intermediate result is available. This coordinate falls under the scope of communication - therefore, channels.

Regarding concurrent goroutines, there's also the case where we want to transfer the ownership of a resource from one step (G1 and G2) to another (G3); for example, if G1 and G2 are enriching a shared resource and at some point; we consider this job as complete. Here, we should use channels to signal that a specific resource is ready and handle the ownership transfer.

Mutexes and channels have different semantics. Whenever we want to share a state or access a shared resource, mutexes ensure exclusive access to this resource. Conversely, channels are mechanic for signaling with or without data (chan struct{} or not). Coordination or ownership transfer should be achived via channels. It's important to know whether goroutines are parallel or concurrent because, in general, we need mutexes for parallel goroutines and channels if concurrent ones.

## 58. Not understand race problems

Race problems can be among the hardest and most insidious bugs a programmer can face. As Go developers, we must understand crucial aspects such as data races and race considitions, their possible impacts, and how to avoid them. We will go through these topics by first dicussing data races versus race conditions and then examining the Go memory model and why it matters.

### Data races vs race conditions

Let's first focus on data races. A data race occurs when two or more goroutines simultaneously access the same memory location and at least one is writing.

```go
i := 0

go func(){
    i++
}()

go func(){
    i++
}()
```

If we run this code using the Go race detector (-race option), it warns us that a data race has occurred:

```txt
WARNING: DATA RACE
Read at 0x00c0000b8018 by goroutine 7:
  main.main.func1()
      main.go:14 +0x30

Previous write at 0x00c0000b8018 by goroutine 8:
  main.main.func2()
      main.go:17 +0x44
```

The final value of i is also unpredictable.

The i++ statement can be decomposed into three operations:

```txt
1. Read i.
2. Increment the value.
3. Write back to i.
```

This is a possible impact of a data race. If two goroutines simultaneously access the same memory location with at least one writing to that memory location, the result can be hazardous.

How can we prevent a data race from happening? Let's look at some different techniques. The scope here isn't to present all the possible options.

The first option is to make the increment operation atomic, meaning it's done in a single operation. This prevents entangled running operations. Atomic operations can be done in Go using the sync/atomic package. Here's an example of how we can increment atomically an int64: 

```go
var i int64

go func(){
    atomic.AddInt64(&i, 1)
}()

go func(){
    atomic.AddInt64(&i, 1)
}()
```

Another option is to synchronize the two goroutines with an ad hoc data structure like a mutex. Mutex stands for mutual exclusion; a mutex ensures that at most one goroutine accesses a so-called critical section. 

```go
i := 0
m := sync.Mutex{}

go func() { 
    m.Lock() // start of critical session
    i++
    m.Unlock() // end of critical session
}()

go func() {
    m.Lock()
    i++
    m.Unlock()
}()
```

Which approach works best? The boundary is pretty straightforward. As we mentioned, the sync/atomic package works only with specific types. If we want something else (for example, slices, maps, and structs), we can't rely on sync/atomic. 

Another possible option is to prevent sharing the same memory location and instead favor communication across goroutines.

```go
i := 0
ch := make(chan int)
go func() {
    ch <- 1
}()
go func() {
    ch <- 1
}()

i += <- ch
i += <- ch
```

Let's sum up what we have seen so far. Data races occur when multiple goroutines access the same memory location simultaneously (for example, the same variable) and at least one of them is writing. We have also seen how to prevent this issue with three synchronization approaches:

- Using atomic operations.
- Protecting a critical section with a mutex.
- Using communication and channels to ensure that a variable is updated by only one goroutine.

With these three approaches, the value of i will eventually be set to 2, regardless of the execution order of the two goroutines. But depending on the operation we want to perform, does a data-race-free application necessarily mena a deterministic result?

Instead of having two goroutines increment a shared variable, now each one makes an assignment. We will follow the approach of using a mutex to prevent data races:

```go
i := 0
m := sync.Mutex{}

go func() {
    m.Lock()
    defer m.Unlock()
    i = 1
}()

go func(){
    m.Lock()
    defer m.Unlock()
    i = 2
}()
```

The first goroutine assgins 1 to i, whereas the second one assigns to 2. This example doesn't lead to a data race. But it has a race condition. A race condition occurs when the behavior depends on the sequence or the timing of events that can't be controlled. Here, the timing of events is the goroutine's execution order.

Ensuring a specific execution sequence among goroutines is a question of coordination and orchestration. If we want to ensure that we first go from state 0 to state 1, and then from state 1 to state 2, we should find a way to guarantee that the goroutines are executed in order. Channels can be a way to solve this problem. Coordinating and orchestrating can also ensure that a particular section is accessed by only one goroutine, which can also mean removing the mutex in the previous example.

In summary, when we work in concurrent applications, it's essential to understand that a data race is different from a race condition. A data race occurs when multiple goroutines simultaneously access the same memory location and at least one of them is writing. A data race means unexpected behavior. However, a data-race-free application doesn't necessarily mean deterministic results. An application can be free of data races but still have behavior that depends on uncontrolled events (such as goroutine execution, how fast a message is published to a channel, or how long a call to a database lasts); this is a race condition. Understanding both concepts is crucial to becoming proficient in designing concurrent applications.

### The Go memory model

The Go memory model is a specification that defines the conditions under which a read from a variable in one goroutine can be guaranteed to happen after a write to the same variable in a different goroutine. In other words, it provides guarantees that developers should keep in mind to avoid data races and force deterministic output.

## 60. Misnderstanding Go contexts

Developers sometimes misunderstand the context.Context type despite it being one of key concepts of the language and a foundation of concurrent code in Go.

A Context carries a deadline, a cancellation signal, and other values across API boundaries. 

### Deadline

A deadline refers to a specific point in time determined with one of the following:

- A time.Duration from now 

#### Context values

The last use case for Go contexts is to carry a key-value list.

A context conveying values can be created this way:

```go
ctx = context.WithValue(parentCtx, "key", "value")
```

Just like context.WithTimeout, context.WithDeadline, and context.WithCancel, context.WithValue is created from a parent context. In this case, we create a new ctx context containing the same characteristics as parentCtx but also conveying a key and a value.

We can access the value using the Value method:

```go
ctx := context.WithValue(context.Background(), "key", "value")
fmt.Println(ctx.Value("key"))
```

Two functions from different packages could use the same string value as a key. Hence, the latter would override the former value. Consequently, a best practice while handling context key is to create an unexported custom type.

```go
type key string

const myCustomKey key = "key"
func f(ctx context.Context) {
    ctx = context.WithValue(ctx, myCustomKey, "foo")
}
```

The myCustomKey constant is unexported. Hence, there's no risk that another package using the same context could override the value that is already set. Even if another package creates the same myCustomKey based on a key type as well, it will be a different key.

#### Catching a context cancellation

The context.Context type exports a Done method that returns a receive-only notification channel: <- chan struct{}. This channel is closed when the work associated with the context should be canceled.


- The Done channel related to a context created with context.WithCancel is closed when the cancel function is called.
- The Done channel related to a context with context.WithDeadline is closed when the deadline has expired.

One thing to note is that the internal channel should be closed when a context is canceled or has met a deadline, instead of when it receives a specific value, because the closure of a channel is the only channel action that all the consumer goroutines will receive. This way, all the consumers will be notified once a context is canceled or a deadline is reached.

Furthermore, context.Context exports an Err method that returns nil if the Done channel isn't yet closed. Otherwise, it returns a non-nil error explaining why the Done channel was closed:

- A context.Canceled error if the channel was canceled 
```go
func main() {
	ctx, cancel := context.WithTimeout(context.Background(), time.Second*1)
	cancel()
	cancelCtx(ctx)
}

func cancelCtx(ctx context.Context) {
	fmt.Printf("ctx.Err() %v\n", ctx.Err())
	select {
	case <-ctx.Done():
		fmt.Println("<-ctx.Done()")
	}
}
```

- A context.DeadlineExceeded error if the context's deadline passed
```go
func main() {
	ctx, cancel := context.WithTimeout(context.Background(), time.Second*1)
	defer cancel()
	time.Sleep(time.Second * 2)
	cancelCtx(ctx)
}

func cancelCtx(ctx context.Context) {
	fmt.Printf("ctx.Err() %v\n", ctx.Err())
	select {
	case <-ctx.Done():
		fmt.Println("<-ctx.Done()")
	}
}
```

Let's see a concrete example in which we want to keep receiving messages from a channel. Meanwhile, our implementation should be context aware and return if the provided context is done:

```go
func handler(ctx context.Context, ch chan Message) error {
    for {
        select {
            case msg := <- ch:
                // Do something with msg
            case <- ctx.Done():
                return ctx.Err() 
        }
    }
}
```

We create a for loop and use select with two cases: receiving messages from ch or receiving a signal that the context is done and we have to stop our job. While dealing with channels, this is an example of how to make a function context aware.

**Implement a function that receives a context**

With a function that receives a context conveying a possible cancellation or timeout, the action of receiving or sending a message to a channel shouldn't be done in a blocking way.

```go
func f(ctx context.Context) error {
    ch1 <- struct{}{}

    v := <- ch2
}
```

The problem with this function is that if the context is canceled or times out, we may have to wait until a message is sent or received, without benefit. Instead, we should use select to either wait for the channel actions to complete or wait for the context cancellation:

```go
func f(ctx context.Context) error {
    select {
        case <- ctx.Done(): 
            return ctx.Err()
        case ch1 <- struct{}{}:
    }
    select {
        case <- ctx.Done(): 
        case v := <-ch2:
    }
}
```

With this new version, if ctx is canceled or timeout, we return immediately, without blocking the channel send or receive.

In Go, context.Context is everywhere in the standard library and external libraries. As we mentioned, a context allows us to carry a deadline, a cancellation signal, and/or a list of keys-values. In general, a function that users wait for should take a context, as doing so allows upstream callers to decide when calling this function should be aborted.

Finally, let's note that the available contexts in the standard library are all safe for concurrent use by multiple goroutines.

### Summary

- Understanding the fundamental differences between concurrency and parallelism is a cornerstone of the Go developer's knowledge. Concurrency is about structure, whereas parallelism is about execution.
- To be a proficient developer, you must acknowledge that concurrency isn't always faster. Solutions involving parallelization of minimal workloads may not necessarily be faster than a sequential versus concurrent solutions should be the way to validate assumptions.
- Being aware of goroutine interactions can also be helpful when deciding between channels and mutexes. In general, parallel goroutines require synchronization and hence mutexes. Conversely, concurrent goroutines generally require coordination and orchestration and hence channels.
- Being proficient in concurrency also means understanding that data races and race conditions are different concepts. Data races occur when multiple goroutines simultaneously access the same memory location and at least one of them is writing. Meanwhile, being data-race-free doesn't necessarily mean deterministic execution. When a behavior depends on the sequence or the timing of events that can't be controlled, this is a race condition.
- Understanding the Go memory model and the underlying guarantees in terms of ordering and synchronization is essential to prevent possible data races and/or race conditions.
- When creating a certain number of goroutines, consider the workload type. Creating CPU-bound goroutines means bounding this number to the GOMAXPROCS variable (based by default on the number of CPU cores on the host). Creating I/O-bound goroutines depends on other factors, such as the external system.
- Go contexts are also one of the cornerstones of concurrency in Go. A context allows you to carry a deadline, a cancellation signal, and/or a list of keys-values.

- A time.Time

The semantics of a deadline convey that an ongoing activity should be stopped if this deadline is met. An activity is, for example, an I/O request or a goroutine waiting to receive a message from a channel.

```go
type publisher interface {
	Publish(ctx context.Context, position int) error
}

type publishHandler struct {
	pub publisher
}

func (h publishHandler) publishPosition(position int) error {
	ctx, cancel := context.WithTimeout(context.Background(), 4*time.Second)
    // ! Ever confirm yet
	// What's the rationale for calling the cancel function as a defer function?
	// Internally, context.WithTimeout creates a goroutine that will be retained in memory for 4 seconds or until cancel
	// is called. Therefore, calling cancel as a defer function means that when we exit the parent function, the context will
	// be canceled, and the goroutine created will be stopped. It's a safeguard so that when we return, we don't leave retianed
	// objects in memory.
	defer cancel()

	return h.pub.Publish(ctx, position)
}
```

### Cancellation signals
