# Testing

**This chapter covers**

- Categorizing tests and making them more robust
- Making Go tests deterministic
- Working with utility packages such as httptest and iotest
- Avoiding common benchmark mistakes

Testing is a crucial aspect of a project's lifecycle. It offers countless benefits, such as building confidence in an application, acting as code documentation, and making refactoring easier. Compared to some other languages, Go has strong primitives for writing tests. Throughout this chapter, we look at common mistakes that make the testing process brittle, less effective, and less accurate.

## 82. Not categorizing tests

The testing pyramid is a model that groups tests into different categories. Unit tests occupy the base on the pyramid. Most tests should be unit tests: they're cheap to write, fast to execute, and highly deterministic. Usually, as we go. Further up the pyramid, tests become more complex to write and slower to run, and it is more difficult to guarantee their deterministic.

A common technique is to be explicit about which kind of tests to run. For instance, depending on the project lifecycle stage, we may want to run only unit tests or run all the tests in the project. Not categorizing tests means potentially wasting time and effort and losing accuracy about the scope of a test. This section discusses three main ways to categorize tests in Go.

**Build tags**

The most common way to classify tests is using build tags. A build tag is a special comment at the beginning of a Go file, followed by an empty file.

```go
//go:build foo
package bar
```

This file contains the `foo` tag. Note that one package may contain multiple files with different build tags.

**Note** As of Go 1.17, the syntax //+build foo was replaced by //go:build foo. For the time being (Go 1.18) gofmt synchronizes the two forms to help with migration.

Build tags are used for two primary use cases. First, we can use a build tag as a conditional option to build an application: for example, if we want a source file to be included only if cgo is enabled (cgo is a way to let Go packages call  C code), we can add the //go:build cgo build tag. Second, if we want to categorize a test as an integration test, we can add a specific build flag, such as integration.

```go
//go:build integration

package db

func TestInsert(t *testing.T) {}
// go test --tags=integration -v .
```

So, running tests with a specific tag includes both the files without tags and the files matching this tag. What if we want to run only integration tests? A possible way is to add a negation tag on the unit test files. For example, using !integration means we want to include the test file only if the integration flag is not enabled.

```go
//go:build !integration

package db

func TestXY(t *testing.T) {}
```

Using this approach,
- Running go test with the intergration flag runs only the integration tests.
- Running go test without the integration flag runs only the unit tests.

**Environment variables**

Built tags have one main drawback: the absence of signals that a test has been ignored. When we executed go test without build flags, it showed only the tests that were executed:

```txt
go test -v
=== RUN TestUnit
--- PASS TestUnit (0.01s)
PASS
```

```go
func Test_add(t *testing.T) {
	if os.Getenv("INTEGRATION") != "true" {
		t.Skip("skipping integration test")
	}
}
```

One benefit of using this approach is making explicit which tests are skipped and why. This technique is probably less widely used than build tags, but it's worth knowing about because it presents some advantags, as we discussed.

**Short mode**

Another approach to categorize tests is related to their speed. We may have to dissociate short-running tests from long-running tests.

As an illustration, suppose we have a set of unit tests, one of which is notoriously slow. We would like to categorize the slow test so we don't have to run it every time (especially if the trigger is after saving a file). Short mode allows us to make this distinction.

```go
func Test_add(t *testing.T) {
	if testing.Short() {
		t.Skip("skipping long-running test")
	}
}
```

Using testing.Short, we can retrieve whether short mode was enabled while running the test. Then we use Skip to skip the test. To run tests using short mode, we have to pass -short.

```txt
go test -short -v
```

In summary, categorizing tests is a best practice for a successful testing strategy. In this section, we've seen three ways to categorize tests:

- Using build tags at the test file level.
- Using env to mark a specific test.
- Based on the test pace using short mode.

## 83. Not enabling the -race flag

In mistake #58, "Not understanding race problems", we defined a data race as occuring when two goroutines simultaneously access the same variable, with at least one writing to the variable. We should also know that Go has a standard race-detector tool to help detect data races. Once common mistake is forgetting how important this tool is and not enabling it. This section looks at what the race detector catches, how to use it, and its limitations.

In Go, the race detector isn't a static analysis tool used during compilation; instead, it's a tool to find data races that occur at runtime. To enable it, we have to enable the -race flag while compiling or running a test. For example:

```bash
go test -race ./...
```

Once the race detector is enabled, the compiler instruments the code  to detect data races. Instrumentation refers to a compiler addion extra instructions: here tracking all memory accesses and recording when and how they occur. At runtime, the race detector watches for data races. However, we should keep in mind the runtime overhead of enabling the race detector:

- Memory usage may increase by 5 to 10x.
- Execution time may increase by 2 to 20x.

Because of this overhead, it's generally recommended to enable the race detector only during local testing or continous integration (CI). In production, we should avoid it (or only use it in the case of canary releases.)

If a race is detected, Go raises a warning. For instance, this example contains a data race because i can be accessed at the same time for a read and a write:

```go
package main

import (
	"fmt"
	"runtime"

	_ "github.com/go-sql-driver/mysql"
)

func makeRace() {
	i := 0
	go func() { i++ }()
	fmt.Println(i)
}
```

Running this application with the -race flag logs the following data race warning:

```txt
==================
WARNING: DATA RACE
Write at 0x00c0000203b8 by goroutine 8:
  go-learn.makeRace.func1()
      /home/ubuntu/code/learning/backend/golang/main.go:12 +0x44

Previous read at 0x00c0000203b8 by goroutine 7:
  go-learn.makeRace()
      /home/ubuntu/code/learning/backend/golang/main.go:13 +0xbe
  go-learn.Test_main()
      /home/ubuntu/code/learning/backend/golang/main_test.go:10 +0x24
  testing.tRunner()
      /usr/local/go/src/testing/testing.go:1576 +0x216
```

Let's make sure we are comfortable reading these messages. Go always logs the following:

- The concurrent goroutines that are incriminated: here, the main goroutine and goroutine 8.
- Where accesses occur in the code: in this case: line 12, and 13.
- When these goroutines were created: goroutine 8 is was created in main().

**NOTE** Internally, the race detector uses vector clocks, a datastructure used to determine a partial ordering of events (and also used in distributed systems such as databases). Each goroutine creation leads to the creation of a vector clock. The instrumentation updates the vector clock at each memory access and synchronization event. Then, it compares the vector clocks to detect potential data races. 

In summary, we should bear in mind that running tests with the -race flag for applications using concurrency is highly recommended, if not mandatory. This approach allows us to enable the race detector, which instruments our code to catch potential data races. While enabled, it has a significant impact on memory and performance, so it must be used in specific conditions such as local tests or CI.

## 84. Not using test execution models

While running tests, the go command can accept a set of flags to impact how tests are executed. A common mistake is not being aware of these flags and missing opportunities that could lead to faster execution or a better way to spot possible bugs. Let's look at two of these flags: parallel and shuffle.

### The parallel flag

Parallel execution mode allows us to run specific tests in parallel, which can be very useful: for example, to speed up long-running tests. We can mark that a test has to be run in parallel by calling t.Parallel:

```go
func TestFoo(t *testing.T) {
    t.Parallel()
}
```

When we mark a test using t.Parallel, it is executed in parallel alongside all the other parallel tests. In terms of execution, though, Go first runs all the sequential tests one by one. Once the sequential tests are completed, it executed the parallel tests.

```go
func TestA(t *testing.T) {
	t.Parallel()
	time.Sleep(time.Second)
	fmt.Println("TestA")
}

func TestB(t *testing.T) {
	t.Parallel()
	time.Sleep(time.Second)
	fmt.Println("TestB")
}

func TestC(t *testing.T) {
	time.Sleep(time.Second)
	fmt.Println("TestC")
}
```

```txt
=== RUN   TestA
=== PAUSE TestA
=== RUN   TestB
=== PAUSE TestB
=== RUN   TestC
TestC
--- PASS: TestC (1.00s)
=== CONT  TestA
=== CONT  TestB
TestB
TestA
--- PASS: TestA (1.00s)
--- PASS: TestB (1.00s)
PASS
```

TestC is the first to be executed. TestA and TestB are logged first, but they are paused, waiting for TestC to complete. Then both are resumed and executed in parallel.

By default, the maximum number of tests that can run simultaneously equals the GOMAXPROCS value. To serialize tests or, for example, increase this number in the context of long-running tests doing a lot of I/O, we can change this value using -parallel flag:

```go
go test -parallel 16 .
```

### The -shuffle flag

As of Go 1.17, it's possible to randomize the execution order of tests and benchmarks. What's the rationale? A best practice while writing tests is to make them isolated. For example, they shouldn't depend on execution order or shared variables. These hidden dependencies can mean a possible test error or, even worse, a bug that won't be caught during testing. To prevent that, we can use the shuffle flag to randomize tests. We can set it to on or off to enable or disable test shuffling (its disabled by default):

```bash
go test -shuffle=on -v .
```

```txt
go test -shuffle=on -v .
-test.shuffle 1701327822122952223
=== RUN   TestB
=== PAUSE TestB
=== RUN   TestA
=== PAUSE TestA
=== RUN   TestC
```

We executed the tests randomly, but go test printed the seed value 1701327822122952223. To force the tests to be run in the same order, we provide this seed value to shuffle:

```txt
go test -shuffle=1701327822122952223 -v .
-test.shuffle 1701327822122952223
=== RUN   TestB
=== PAUSE TestB
=== RUN   TestA
=== PAUSE TestA
=== RUN   TestC
```

The tests were executed in the same order: TestBar and then TestFoo.

In general, we should be cautions about existing test flags and keep ourselves informed about new features with recent Go releases. Running tests in parallel can be an excellent way to decrease the overall execution time of running all the tests. And shuffle mode can help us spot hidden dependencies that may mean testing errors or even invisible bugs while running tests in the same order.

## 85. Not using table-driven tests

Table-driven tests are an efficient technique for writing condensed tests and thus reducing boilerplate code to help us focus on what matters: the testing logic. This section goes through a concrete example to see why table-driven tests are worth knowing when working with Go.

Let's consider the following function that removes all the new-line suffixes (\n or \r\n) from a string:

```go
func removeNewLineSuffixes(s string) string {
	if s == "" {
		return s
	}
	if strings.HasSuffix(s, "\r\n") {
		return removeNewLineSuffixes(s[:len(s)-2])
	}
	if strings.HasSuffix(s, "\n") {
		return removeNewLineSuffixes(s[:len(s)-1])
	}
	return s
}
```

This function removes all the leading \r\n and \n suffixes recursively. Now, let's say we want to test this function extensively. We should at least cover the following cases:

- Input is empty.
- Input ends with \n.
- Input ends with \r\n.
- Input ends with multiple \n.
- Input ends without newlines.

The following approach creates one unit test per case:

```go
func TestRemoveNewLineSuffix_Empty(t *testing.T) {
    got := removeNewLineSuffixes("")
    expected := ""
    if got != expected {
        t.Errorf("got: %s", got)
    }
}

func TestRemoveNewLineSuffix_EndingWithCarriageReturnNewLine(t *testing.T) {
    got := removeNewLineSuffixes("a\r\n")
    expected := "a"
    if got != expected {
        t.Errorf("got: %s", got)
    }
}

// ...
```

Each function represents a specific case that we want to cover. However, there are two main drawbacks. First, the function names are more complex.

Instead, we can use table-driven tests so we write the logic only once. Table-driven tests rely on subtests, and a single test function can include multiple subtests.

In summary, if multiple unit tests have a similar structure, we can multualize them using table-driven tests. Because this technique prevents duplication, it makes it simple to change the testing logic and easier to add new use cases.

## 86. Sleeping in uint tests

A flaky test is a test that may both pass and fail without any code change. Flaky tests are among the biggest hurdles in testing because they are expensive to debug and undermine our confidence in testing accuracy. In Go, calling time.Sleep in a test can be a signal of possible flakiness. For example, concurrent code is often tested using sleeps. This section presents concrete techniques to remove sleeps from tests and thus prevent us from writing flaky tests.

```go
type Handler struct {
	n         int
	publisher publisher
}

type publisher interface {
	Publish([]any)
}

func (h Handler) getBestFoo(someInputs int) any {
	foos := getFoos()
	best := foos[0]

	go func() {
		if len(foos) > h.n {
			foos = foos[:h.n]
		}
		h.publisher.Publish(foos)
	}()

	return best
}

func getFoos() []any {
	ar := []int{1, 2, 3}
	var rs []any
	for _, v := range ar {
		rs = append(rs, v)
	}
	return rs
}
```

The Handler struct contains two fields: an n field and a publisher dependency used to publish the first n Foo structs. First we get a slice of Foo; but before returning the first element, we spin up a new goroutine, filter the foos slice, and call Publish.

How can we test this function? Writing the part to assert the response is straightforward. However, what if we also want to check what is passed to Publish?

We could mock the publisher interface to record the arguments passed while calling the Publish method. Then we could sleep for a few milliseconds before checking the arguments recorded:

```go
type publisherMock struct {
    mu sync.Mutex
    got []Foo
}

func (p *publisherMock) Publish(got []Foo) {
    p.mu.Lock()
    defer p.mu.Unlock()
    p.got = got
}

func (p *publisherMock) Get() []Foo {
    p.mu.Lock()
    defer p.mu.RUnlock()
    return p.get
}

func TestGetBestFoo(t *testing.T) {
    mock := publisherMock{}
    h := Handler{
        publisher: &mock,
        n: 2,
    }

    foo := h.getBestFoo(42)
    // Check Foo

    time.Sleep(time.Millisecond * 10)
    published := mock.Get()
    // Check published
}
```

## 87. Not dealing with the time API efficiently

Some functions have to rely on the time API: for example, to retrieve the current time. In such a case in can be pretty easy to write brittle unit tests that may fail at some point. The goal is not to cover every use case and technique but rather to give directions about writing more robust tests of functions using the time API.

Each of these methods needs to access the current time. Let's write a first implementation of the third method using time.Now() (we will assume that all the events are sorted by time).

```go
type Event struct {
	Timestamp time.Time
	Data      string
}

type Cache struct {
	mu     sync.RWMutex
	events []Event
}

func (c *Cache) TrimOlderThan(since time.Duration) {
	c.mu.RLock()
	defer c.mu.RUnlock()

	t := time.Now().Add(-since)
	for i := 0; i < len(c.events); i++ {
		if c.events[i].Timestamp.After(t) {
			c.events = c.events[i:]
			return
		}
	}
}
```

## 89. Not using testing utility packages

The standard library provides utility packages for testing. A common mistake is being unaware of these packages and trying to reinvent the wheel or rely on other solutions that aren't as handy. This section examines two of these packages: one to help us when using HTTP and another to use when doing I/O and using readers and writers.

### The httptest package

The httptest package provides utilities for HTTP testing for both clients adn servers. Let's look at these  two use cases.

First, let's see how httptest can help us while writing an HTTP server. We will implement a handler that performs some basic actions: writing a header and body, and returning a specific status code. For the sake of clarity, we will omit error handling:

```go
func Handler(w http.ResponseWriter, r *http.Request) {
    w.Header().Add("X-API-VERSION", "1.0")
    b, _ := io.ReadAll(r.Body)
    _, _ = w.Write(append([]byte("hello"), b...))
    w.WriteHeader(http.StatusCreated)
}
```

An HTTP handler accepts two arguments: the request and a way to write the response. The httptest package provides utilities for both. For the request, we can use http-test. NewRequest to build an *http.Request using an HTTP method, a URL, and a body. For the response, we can use httptest.NewRecorder to record the mutations made within the handler. Let's write a unit test of this handler:

```go
func TestHandler(t *testing.T) {
	req := httptest.NewRequest(http.MethodGet, "http://localhost", strings.NewReader("foo"))
	w := httptest.NewRecorder()
	Handler(w, req)

	if got := w.Result().Header.Get("X-API-VERSION"); got != "1.0" {
		t.Errorf("api version: expected 1.0, got %s", got)
	}

	body, _ := io.ReadAll(w.Body)
	if got := string(body); got != "hello foo" {
		t.Errorf("body: expected hello foo, got %s", got)
	}

	if http.StatusOK != w.Result().StatusCode {
		t.FailNow()
	}
}
```

Testing a handler using httptest doesn't test the transport (the HTTP part). The focus of the test is calling the handler directly with a request and a way to record the response. Then, using the response recorder, we write the assertions to verify the HTTP header, body, and status code.

### The iotest package

## 89. Writing inaccurate benchmarks

In general, we should never guess about performance. When writing optimizations, so many factors may come into play that even if have a strong opinion about the results, it's rarely a bad idea to test them. However, writing benchmarks isn't straight-forward. It can be pretty simple to write inaccurate benchmarks and make wrong assumptions based on them. The goal of this section is to examine common and concrete traps leading to inaccuracy.

### Not resetting or pausing the timer

In some case, we need to perform operations before the benchmark loop. These operations may take quite a while (for example, generating a large slice of data) and may significant impact the benchmark results:

```go
func BenchmarkFoo(b *testing.B) {
    expensiveSetup()
    for i := 0; i < b.N; i++ {
        functionUnderTest()
    }
}
```

In this case, we can use the ResetTimer method before entering the loop:

```go
func BenchmarkFoo(b *testing.B) {
    expensiveSetup()
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        functionUnderTest()
    }
}
```

Calling ResetTimer zeroes the elapsed benchmark time and memory allocation counters since the beginning of the test. This way, an expensive setup can be discarded from the test results.

What if we have to perform an expensive setup not just once but within each loop function?

```go
func BenchmarkFoo(b *testing.B) {
    for i := 0; i < b.N; i++ {
        expensiveSetup()
        functionUnderTest()
    }
}
```

We can't reset the timer, because that would be executed during each loop iteration. But we can stop and resume the benchmark timer, surrounding the call to expensiveSetup:

```go
func BenchmarkFoo(b *testing.B) {
    for i := 0 ; i < b.N; i++ {
        b.StopTimer() // Pauses the benchmark timer
        expensiveSetup()
        b.StartTimer() // Resumes the benchmark timer
        functionUnderTest()
    }
}
```

Here we pause the benckmark timer to perform the expensive setup and then resume the timer.

**NOTE** There's one catch to remember about this approach: if the function under test is too fast to execute compared to the setup function, the bench-mark may take too long to complete. The reason is that it would take much longer than 1 second to reach the benchmarktime. Calculating the benchmark time is based solely on the execution time of the functionUnderTest. So, if we wait a significant time in each loop iteration, the benchmark will be much slower than 1 second. If we want to keep the benchmark, one possible mitigation is to decrease benchtime.

### Marking wrong assumptions about micro-benchmarks

A micro-benchmark measures a tiny computation unit, and it can be extremely easy to make wrong assumptions about it. Let's say, for example, that we aren't sure whether to use atomic.StoreInt32 or atomic.StoreInt64 (assuming that the values we handle will always fit in 32 bits). We want to write a benchmark to compare both functions. 

```go
func BenchmarkAtomicStoreInt32(b *testing.B) {
    var v int32
    for i := 0; i < b.N ; i++ {
        atomic.StoreInt32(&v, 1)
    }
}

func BenchmarkAtomicStoreInt64(b *testing.B) {
    var v int64
    for i := 0; i < b.N ; i++ {
        atomic.StoreInt64(&v, 1)
    }
}
```

```txt
cpu: 12th Gen Intel(R) Core(TM) i7-1255U
BenchmarkAtomicStoreInt32
BenchmarkAtomicStoreInt32-12            317838268                3.792 ns/op
BenchmarkAtomicStoreInt64
BenchmarkAtomicStoreInt64-12            305561109                3.731 ns/op
```

We could easily take this benchmark for granted and decide to use atomic.StoreInt64 because it appears to be faster. Now, for the sake of doing a fair benchmark, we reverse the order and test atomic.StoreInt64 first, followed by atomic.StoreInt32.

```txt
cpu: 12th Gen Intel(R) Core(TM) i7-1255U
BenchmarkAtomicStoreInt64
BenchmarkAtomicStoreInt64-12            319730383                3.713 ns/op
BenchmarkAtomicStoreInt32
BenchmarkAtomicStoreInt32-12            316451656                3.765 ns/op
```

This time, atomic.StoreInt32 has better results. What happend?

In the case of micro-benchmarks, many factors can impact the results, such as machine activity while running the benchmarks, power management, thermal scaling, and better cache alignment of a sequence of instructions. We must remember that many factors, even outside the scope of our Go project, can impact the results.

**NOTE** We should make sure the machine executing the benchmark is idle. However, external processes may run in the background, which may affect benchmark results. For that reason, tools such as perflock can limit how much CPU a benchmark can consume. For example, we can run a benchmark with 70% of the total available CPU, giving 30% to the OS and other processes and reducing the impact of the machine activity factor on the results.

One option is to increase the benchmark time using the -benchtime option. Similar to the law of large numbers in probability theory, if we run a benchmark a large number of times, it should tend to approach its expected value (assuming we omit the benefits of instructions caching and similar mechnics).

Another option is to use external tools on top of the classic benchmark tooling. For instance, the benchstat tool, which is part of the golang.org/x repository, allows us to compute and compare statistics about benchmark executions. 

In general, we should be cautions about micro-benchmarks. Many factors can significantly impact the results and potentially lead to wrong assumptions. Increasing the benchmark time or repeating the benchmark executions and computing stats with tools such as benchstat can be an efficient way to limit external factors and get more accurate results, leading to better conclusions.

### Not being careful about compiler optimizations

Another common mistake related to writing benchmarks is being fooled by compiler optimizations, which can also lead to wrong benchmark assumptions. 

```go
const m1 = 0x5555555555555555
const m2 = 0x3333333333333333
const m4 = 0x0f0f0f0f0f0f0f0f
const h01 = 0x0101010101010101

func popcnt(x uint64) uint64 {
	x -= (x >> 1) & m1
	x = (x & m2) + ((x >> 2) & m2)
	x = (x + (x >> 4)) & m4
	return (x * h01) >> 56
}
```

```go
func Benchmark_popcnt(b *testing.B) {
	for i := 0; i < b.N; i++ {
		popcnt(11)
	}
}
```

```txt
cpu: 12th Gen Intel(R) Core(TM) i7-1255U
Benchmark_popcnt
Benchmark_popcnt-12     1000000000               0.1194 ns/op
```

0.1194 ns/op is roughly one clock cycle, so this number is unreasonably low. The problem is that the developer wasn't careful enough about compiler optimizations. 

### Being fooled by the observer effect

## 90. Not exploring all the Go testing features

When it comes to writing tests, developers should know about Go's specific testing features and options.

### Code coverage
