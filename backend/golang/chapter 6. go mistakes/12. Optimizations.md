# Optimizations

Before we begin this chapter, a disclaimer: in most contexts, writing readable, clear code is better than writing code that is optimized but more complex and difficult to understand. Optimization generally comes with a price, and we advocate that you follow this famous quote from software engineer Wes Dyer:

**Make it correct, make it clear, make it concise, make it fast, in that order.**

## 91. Not understanding CPU caches

You don't have to be an engineer to be a racing driver but you do have to have mechnical sympathy

### CPU architecture

Modern CPUs rely on caching to speed up memory access, in most cases via three caching levels: L1, L2 and L3. On the i5-7300, here are the sizes of these caches:

- L1: 64 KB
- L2: 256 KB
- L3: 4 MB

The i5-7300 has two physical cores but four logical cores (also called virtual cores or threads). In the Intel family, dividing a physical core into multiple logical cores is called Hyper-Threading.

Each physical core (core 0 and core 1) is divided into two logical cores (thread 0 and thread 1). The L1 cache is split into two sub-caches: L1D for data and L1L instructions (each 32 KB). Caching isn't solely related to data - when a CPU executes an application, it can also cache some instructions with the same rationale: to speed up overall execution.

![cpu cache](./assets/cpu_cache.png)

The physical location of the CPU caches can also explain these differences. L1 and L2 are called on-die, meaning they belong to the same piece of silicon as the rest of processor. Conversely, L3 is off-die, which partly explains the latency differences compared to L1 and L2.

For main memory (or RAM), average accesses are between 50 and 100 times slower than L1. We can access up to 100 variables stored on L1 for the price of a single access to the main memory. Therefore, as Go developers, one avenue for improvement is making sure our applications use CPU caches.

#### Cache line

The concept of cache lines is crucial to understand. But before presenting what they are, let's understand why we need them.

When a specific memory location is accessed (for example, by reading a variable), one of the following is likely to happen in the near future:

- The same location will be referenced again.
- Nearby memory locations will be referenced.

The former refers to temporal locality, and the latter referes to spatial locality. Both are part of a principle called locality of reference.

```go
func sum(s []int64) int64 {
	var total int64
	length := len(s)
	for i := 0; i < length; i++ {
		total += s[i]
	}
	return total
}
```

In this example, temporal locality applies to multiple variables: i, length, and total. Spatial locality applies to code instructions and the slice s. Because a slice is backed by an array allocated contiguously in memory, in this case, access a[0] means also accessing s[1], s[2], and so on.

Temporal locality is part of why we need CPU caches: to speed up repeated accesses to the same variables. However, because of spatial locality, the CPU copies what we call a cache line instead of copying a single variable from the main memory to a cache.

A cache line is a contiguously memory segment of a fixed size, usually 64 bytes (8 int64 variables). Whenever a CPU decides to cache a memory block from RAM, it copies the memory block to a cache line. Because memory is a hierarchy, when the CPU wants to access a specific memory location, it first checks in L1, then L2, then L3, and finally, if the location is not in those caches, in the main memory.

Let's illustrate fetching a memory block with a concrete example. We call the sum function with a slice of 16 int64 elements for the first time. When sum accesses s[0], this memory address isn't in the cache yet. If the CPU decides to cache this variable (we also discuss this decision later in the chapter), it copies the whole memory block. 

![cache line]('./assets/cache_line.png')

Access s[0] makes the CPU copy the 0x000 memory block.

At first, accessing s[0] results in a cache miss because the compulsory miss. However, if the CPU fetches the 0x000 memory block, accessing elements from 1 to 7 results in a cache hit.

**CPU caching strategies**

You may wonder about the exact strategy when a CPU copies a memory block. For example, will it copy a block to all the levels? Only to L1? In this case, what about L1 and L3?

We have to know that different strategies exist. Sometimes caches are inclusive (for example, L2 data is also present in L3), and sometimes caches are exclusive (for example, L3 is called a victim cache because it contains only data evicted from L2).

Let's keep discussing locality of reference and see a concrete example of using spatial locality. 

#### Slice of structs vs struct of slices 

This section looks at an example that compares the execution time of two functions. The first takes as an argument a slice of struct and sums all the a fields:

```go
type Foo struct {
	a int64
	b int64
}

func sumFoo(foos []Foo) int64 {
	var total int64
	for i := 0; i < len(foos); i++ {
		total += foos[i].a
	}

	return total
}

type Bar struct {
	a []int64
	b []int64
}

func sumBar(bar Bar) int64 {
	var total int64
	for i := 0; i < len(bar.a); i++ {
		total += bar.a[i]
	}
	return total
}
```

In the case of sumFoo, we receive a slice of structs containing two fields, a and b. Therefore, we have a succession of a and b in memory. Conversely, in the case of sumBar, we receive a struct containing two slices, a and b. Therefore, all the elements of a are allocated contiguously.

This difference doesn't lead to any memory compaction optimization. But the goal of both functions is to iterate over each a, and doing so requires four cache lines in one case and only two cache lines in the other.

If we benchmark these two functions, sumbar is faster (about 20% on my machine). The main reason is a better spatial locality that makes the CPU fetch fewer cache lines from memory.

This example demonstrates how spatial locality can have a substantial impact on performance. To optimize an application, we should organize data to get the most value out of each individual cache line.

```go
func Benchmark_sumOfSliceStruct(b *testing.B) {
	foos := make([]Foo, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		foos[i] = Foo{
			a: i,
			b: i,
		}
	}

	for i := 0; i < b.N; i++ {
		sumFoo(foos)
	}
}

func Benchmark_sumOfStructSlice(b *testing.B) {
	a := make([]int64, 1000)
	a1 := make([]int64, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		a[i] = i
		a1[i] = i
	}
	bar := Bar{
		a: a,
		b: a1,
	}

	for i := 0; i < b.N; i++ {
		sumBar(bar)
	}
}
```

However, is using spatial locality enough to help the CPU? We are still missing one crucial characteristic: predictability.

#### Predictability

Predictability refers to the ability of a CPU to anticipate what the application will do to speed up its execution. Let's see a concrete example where a lack of predictability negatively impacts application performance.

Again, let's look at two functions that sum a list of elements. The first iterates over a linked list and sums all the values:

```go
type node struct {
	next  *node
	value int64
}

func linkedlist(n *node) int64 {
	var total int64
	for n != nil {
		total += n.value
		n = n.next
	}
	return total
}
```

This function receives a linked list, iterates over it, and increments a total.

On the other side, let's again take the sum2 function that iterates over a slice, one element out of two:

```go
func sum2(s []int64) int64 {
	var total int64
	for i := 0; i < len(s); i++ {
		total += s[i]
	}
	return total
}
```

Let's assume that the linked list is allocated contiguously: for example, by a single function. On a 64-bit architecture, a world is 64 bits long. The two data structures that functions receive (linked list or slice); the darker bars represent the int64 elements we use to increment the total. In both examples, we face similar compaction. Because a linked list is a succession of values and 64-bit pointer elements, we increment the sum using one element out of two.

The two data structures have the same spatial locality, so we may expect a similar execution time for these two functions. But the function interating on the slice is significant faster (about 70% on my machine). What's the reason?

To understand this, we have to discuss the concept of striding. Striding relates to how CPUs work through data. There are three different types of strides:

- Unit stride: All the values we want to access are allocated contiguously: for example, a slice of int64 elements. This stride is predictable for a CPU and the most efficient because it requires a minimum number of cache lines to walk through the elements.

- Constant stride: Still predictable for the CPU: for example, a slice that iterates over every two elements. This stride requires more cache lines to walk through data, so it's less efficient than a unit stride.

- Non-unit stride: A stride the CPU can't predict: for example, a linked list or slice of pointers. Because the CPU doesn't know whether data is allocated contiguously, it won't fetch any cache lines.

![predictable](./assets/predictable.png)
*The three types of strides*

For `sum2`, we face a constant stride. However, for the linked list, we face a non-unstride. Even though we know the data is allocated contiguously, the CPU doesn't known that. Therefore, it can't predict how to walk through the linked list.

Because the different stride and similar spatial locality, iterating over a linked list is significantly slower than a slice of values. We should generally favor unit strides over constant strides because of the better spatial locality. But a non-unit stride cannot be predicted by the CPU regardless of how the data is allocated, leading to negative performance impacts.

So far, we have discussed that CPU caches are fast but significantly smaller than the main memory. Therefore, a CPU needs a strategy to fetch a memory block to a cache line.

### Cache placement policy

```go
func calculateSum512(s [][512]int64) int64 {
	var sum int64
	for i := 0; i < len(s); i++ {
		for j := 0; j < 8; j++ {
			sum += s[i][j]
		}
	}

	return sum
}

func calculateSum513(s [][513]int64) int64 {
	var sum int64
	for i := 0; i < len(s); i++ {
		for j := 0; j < 8; j++ {
			sum += s[i][j]
		}
	}

	return sum
}
```

TODO: read again

## 92. Writing concurrent code that leads to false sharing

So far, we have dicussed the fundamental concepts of CPU caching. We have seen that some specific caches (typically L1 and L2) aren't shared among all the logical cores but are specific to a physical core. This specificity has some concrete impacts such as concurrency and the concept of false sharing, which can lead to significantly performance decrease. Let's look at what false sharing is via an example and then see how to prevent it.

```go
type Input struct {
    a int64
    b int64
}

type Result struct {
    sumA int64
    sumB int64
}
```

The goal is to implement a count function that receives a slice of Input and computes the following:

- The sum of all the Input.a fields into Result.sumA
- The sum of all the Input.b fields into Result.sumB

For the sake of the example, we implement a concurrent solution with one goroutine that computes sumA and another that computes sumB:

```go
func count(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	result := Result{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return result
}
```

We spin up two goroutines: one that iterates over each a field and another that iterates over each b field. This example is fine from a concurrency perspective. For instance, it doesn't lead to a data race, because each goroutine increments its own variable. But this example illustrates the false sharing concept that degrades expected performance.

Because sumA and sumB are allocated contiguously, in most cases (seven out of eight), both variables are allocated to the same memory block.

Now, let's assume that the machine contains two cores. In most cases, we should eventually have two threads scheduled on different cores. So if the CPU decides to copy this memory block to a cache line, it is copied twice

![fetch cache](./assets/cache-L1D.png)
*Each block is copied to a cache line on both core 0 and core 1.*

Both cache lines are replicated because L1D (L1 data) is per core. Recall that in our example, each goroutine updates its own variable: sumA on one side, and sumB on the other side.

![copy on cache line](./assets/copy-on-cache-line.png)
*Each goroutine updates its own variable.*

Because these cache lines are replicated, one of the goals of the CPU is to guarantee cache coherency. For example, if one goroutine updates sumA and another reads sumA (after some synchronization), we expect our application to get the latest value.

However, our example doesn't do exactly this. Both goroutines access their own variables, not a shared one. We might expect the CPU to know about this and understand that it isn't a conflict, but this isn't the case. When we write a variable that's in a cache, the granularity tracked by the CPU isn't the variable: it's the cache line. 

When a cache line is shared across multiple cores and at least one goroutine is a writer, the entire cache line is invalidated. This happens even if the updates are logically independent (for example, sumA and sumB). This is the problem of false sharing, and it degrades performance.

**NOTE** Internally, a CPU uses the MESI protocol to guarantee cache coherency. It tracks each cache line, marking it modified, exclusive, shared, or invalid (MESI).

One of the most important aspects to understand about memory and caching is that sharing memory across cores isn't real - it's an illusion. This understanding comes from the fact that we don't consider a machine a black box; instead, we try to have mechanical sympathy with underlying levels.

So how do we solve false sharing? There are two main solutions.

The first solution is to use the same approach we've shown but ensure that sumA and sumB aren't part of the same cache line. For example, we can update the Result struct to add padding between the fields. Padding is a technique to allocate extra memory. Because int64 requries 8-byte allocation and a cache line 64 bytes long, we need 64 - 8 = 56 bytes of paddings

```go
type Input struct {
	a int64
	b int64
}
type Result struct {
	sumA int64
	sumB int64
}
type Result2 struct {
	sumA int64
	_    [56]byte // padding
	sumB int64
}

func count1(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	result := Result{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return result
}
func count2(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	chA, chB := make(chan int64, 1), make(chan int64, 1)

	go func() {
		defer wg.Done()
		var sumA int64
		for i := 0; i < len(inputs); i++ {
			sumA += inputs[i].a
		}
		chA <- sumA
	}()

	go func() {
		defer wg.Done()
		var sumB int64
		for i := 0; i < len(inputs); i++ {
			sumB += inputs[i].b
		}
		chB <- sumB
	}()

	wg.Wait()
	return Result{
		sumA: <-chA,
		sumB: <-chB,
	}
}

func count3(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	rs := Result{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			rs.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			rs.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return rs
}
```

![memory padding cacheline](./assets/memory-padding-cacheline.png)
*A possible memory allocation. Using padding, sumA and sumB will always be part of different memory blocks and hence different cache lines.*

If we benchmark both solutions (with and without padding), we see that the padding solution is significantly faster (about 40% on my machine). This is an important improvement that results from the addition of padding between the two fields to prevent false sharing.

The second solution is to rework the structure of the algorithm. For example, instead of having both goroutines share the same struct, we can make them communicate their local result via channels. The result benchmark is roughly the same as with padding.

```go
goos: linux
goarch: amd64
pkg: go-learn
cpu: 12th Gen Intel(R) Core(TM) i7-1255U
Benchmark_count1
Benchmark_count1-12          889           1299806 ns/op
Benchmark_count2
Benchmark_count2-12         1130           1063815 ns/op
Benchmark_count3
Benchmark_count3-12         1130           1053121 ns/op
```

In summary, we must remember that sharing memory across goroutines is an illusion at the lowest memory levels. False sharing occurs when a cache line is shared across two cores when at least one goroutine is a writer. If we need to optimize an application that relies on concurrency, we should check whether false sharing applies because this pattern is known to degrade application performance. We can prevent false sharing with either padding or communication.

The following section discusses how CPUs can execute instructions in parallel and how to leverage that capability.

## 93. Not taking into account instruction-level parallelism


