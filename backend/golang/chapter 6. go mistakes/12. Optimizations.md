# Optimizations

Before we begin this chapter, a disclaimer: in most contexts, writing readable, clear code is better than writing code that is optimized but more complex and difficult to understand. Optimization generally comes with a price, and we advocate that you follow this famous quote from software engineer Wes Dyer:

**Make it correct, make it clear, make it concise, make it fast, in that order.**

## 91. Not understanding CPU caches

You don't have to be an engineer to be a racing driver but you do have to have mechnical sympathy

### CPU architecture

Modern CPUs rely on caching to speed up memory access, in most cases via three caching levels: L1, L2 and L3. On the i5-7300, here are the sizes of these caches:

- L1: 64 KB
- L2: 256 KB
- L3: 4 MB

The i5-7300 has two physical cores but four logical cores (also called virtual cores or threads). In the Intel family, dividing a physical core into multiple logical cores is called Hyper-Threading.

Each physical core (core 0 and core 1) is divided into two logical cores (thread 0 and thread 1). The L1 cache is split into two sub-caches: L1D for data and L1L instructions (each 32 KB). Caching isn't solely related to data - when a CPU executes an application, it can also cache some instructions with the same rationale: to speed up overall execution.

![cpu cache](./assets/cpu_cache.png)

The physical location of the CPU caches can also explain these differences. L1 and L2 are called on-die, meaning they belong to the same piece of silicon as the rest of processor. Conversely, L3 is off-die, which partly explains the latency differences compared to L1 and L2.

For main memory (or RAM), average accesses are between 50 and 100 times slower than L1. We can access up to 100 variables stored on L1 for the price of a single access to the main memory. Therefore, as Go developers, one avenue for improvement is making sure our applications use CPU caches.

#### Cache line

The concept of cache lines is crucial to understand. But before presenting what they are, let's understand why we need them.

When a specific memory location is accessed (for example, by reading a variable), one of the following is likely to happen in the near future:

- The same location will be referenced again.
- Nearby memory locations will be referenced.

The former refers to temporal locality, and the latter referes to spatial locality. Both are part of a principle called locality of reference.

```go
func sum(s []int64) int64 {
	var total int64
	length := len(s)
	for i := 0; i < length; i++ {
		total += s[i]
	}
	return total
}
```

In this example, temporal locality applies to multiple variables: i, length, and total. Spatial locality applies to code instructions and the slice s. Because a slice is backed by an array allocated contiguously in memory, in this case, access a[0] means also accessing s[1], s[2], and so on.

Temporal locality is part of why we need CPU caches: to speed up repeated accesses to the same variables. However, because of spatial locality, the CPU copies what we call a cache line instead of copying a single variable from the main memory to a cache.

A cache line is a contiguously memory segment of a fixed size, usually 64 bytes (8 int64 variables). Whenever a CPU decides to cache a memory block from RAM, it copies the memory block to a cache line. Because memory is a hierarchy, when the CPU wants to access a specific memory location, it first checks in L1, then L2, then L3, and finally, if the location is not in those caches, in the main memory.

Let's illustrate fetching a memory block with a concrete example. We call the sum function with a slice of 16 int64 elements for the first time. When sum accesses s[0], this memory address isn't in the cache yet. If the CPU decides to cache this variable (we also discuss this decision later in the chapter), it copies the whole memory block. 

![cache line](./assets/cache_line.png)

Access s[0] makes the CPU copy the 0x000 memory block.

At first, accessing s[0] results in a cache miss because the compulsory miss. However, if the CPU fetches the 0x000 memory block, accessing elements from 1 to 7 results in a cache hit.

**CPU caching strategies**

You may wonder about the exact strategy when a CPU copies a memory block. For example, will it copy a block to all the levels? Only to L1? In this case, what about L1 and L3?

We have to know that different strategies exist. Sometimes caches are inclusive (for example, L2 data is also present in L3), and sometimes caches are exclusive (for example, L3 is called a victim cache because it contains only data evicted from L2).

Let's keep discussing locality of reference and see a concrete example of using spatial locality. 

#### Slice of structs vs struct of slices 

This section looks at an example that compares the execution time of two functions. The first takes as an argument a slice of struct and sums all the a fields:

```go
type Foo struct {
	a int64
	b int64
}

func sumFoo(foos []Foo) int64 {
	var total int64
	for i := 0; i < len(foos); i++ {
		total += foos[i].a
	}

	return total
}

type Bar struct {
	a []int64
	b []int64
}

func sumBar(bar Bar) int64 {
	var total int64
	for i := 0; i < len(bar.a); i++ {
		total += bar.a[i]
	}
	return total
}
```

In the case of sumFoo, we receive a slice of structs containing two fields, a and b. Therefore, we have a succession of a and b in memory. Conversely, in the case of sumBar, we receive a struct containing two slices, a and b. Therefore, all the elements of a are allocated contiguously.

This difference doesn't lead to any memory compaction optimization. But the goal of both functions is to iterate over each a, and doing so requires four cache lines in one case and only two cache lines in the other.

If we benchmark these two functions, sumbar is faster (about 20% on my machine). The main reason is a better spatial locality that makes the CPU fetch fewer cache lines from memory.

This example demonstrates how spatial locality can have a substantial impact on performance. To optimize an application, we should organize data to get the most value out of each individual cache line.

```go
func Benchmark_sumOfSliceStruct(b *testing.B) {
	foos := make([]Foo, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		foos[i] = Foo{
			a: i,
			b: i,
		}
	}

	for i := 0; i < b.N; i++ {
		sumFoo(foos)
	}
}

func Benchmark_sumOfStructSlice(b *testing.B) {
	a := make([]int64, 1000)
	a1 := make([]int64, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		a[i] = i
		a1[i] = i
	}
	bar := Bar{
		a: a,
		b: a1,
	}

	for i := 0; i < b.N; i++ {
		sumBar(bar)
	}
}
```

However, is using spatial locality enough to help the CPU? We are still missing one crucial characteristic: predictability.

#### Predictability

Predictability refers to the ability of a CPU to anticipate what the application will do to speed up its execution. Let's see a concrete example where a lack of predictability negatively impacts application performance.

Again, let's look at two functions that sum a list of elements. The first iterates over a linked list and sums all the values:

```go
type node struct {
	next  *node
	value int64
}

func linkedlist(n *node) int64 {
	var total int64
	for n != nil {
		total += n.value
		n = n.next
	}
	return total
}
```

This function receives a linked list, iterates over it, and increments a total.

On the other side, let's again take the sum2 function that iterates over a slice, one element out of two:

```go
func sum2(s []int64) int64 {
	var total int64
	for i := 0; i < len(s); i++ {
		total += s[i]
	}
	return total
}
```

Let's assume that the linked list is allocated contiguously: for example, by a single function. On a 64-bit architecture, a world is 64 bits long. The two data structures that functions receive (linked list or slice); the darker bars represent the int64 elements we use to increment the total. In both examples, we face similar compaction. Because a linked list is a succession of values and 64-bit pointer elements, we increment the sum using one element out of two.

The two data structures have the same spatial locality, so we may expect a similar execution time for these two functions. But the function interating on the slice is significant faster (about 70% on my machine). What's the reason?

To understand this, we have to discuss the concept of striding. Striding relates to how CPUs work through data. There are three different types of strides:

- Unit stride: All the values we want to access are allocated contiguously: for example, a slice of int64 elements. This stride is predictable for a CPU and the most efficient because it requires a minimum number of cache lines to walk through the elements.

- Constant stride: Still predictable for the CPU: for example, a slice that iterates over every two elements. This stride requires more cache lines to walk through data, so it's less efficient than a unit stride.

- Non-unit stride: A stride the CPU can't predict: for example, a linked list or slice of pointers. Because the CPU doesn't know whether data is allocated contiguously, it won't fetch any cache lines.

![predictable](./assets/predictable.png)
*The three types of strides*

For `sum2`, we face a constant stride. However, for the linked list, we face a non-unstride. Even though we know the data is allocated contiguously, the CPU doesn't known that. Therefore, it can't predict how to walk through the linked list.

Because the different stride and similar spatial locality, iterating over a linked list is significantly slower than a slice of values. We should generally favor unit strides over constant strides because of the better spatial locality. But a non-unit stride cannot be predicted by the CPU regardless of how the data is allocated, leading to negative performance impacts.

So far, we have discussed that CPU caches are fast but significantly smaller than the main memory. Therefore, a CPU needs a strategy to fetch a memory block to a cache line.

### Cache placement policy

```go
func calculateSum512(s [][512]int64) int64 {
	var sum int64
	for i := 0; i < len(s); i++ {
		for j := 0; j < 8; j++ {
			sum += s[i][j]
		}
	}

	return sum
}

func calculateSum513(s [][513]int64) int64 {
	var sum int64
	for i := 0; i < len(s); i++ {
		for j := 0; j < 8; j++ {
			sum += s[i][j]
		}
	}

	return sum
}
```

TODO: read again

## 92. Writing concurrent code that leads to false sharing

So far, we have dicussed the fundamental concepts of CPU caching. We have seen that some specific caches (typically L1 and L2) aren't shared among all the logical cores but are specific to a physical core. This specificity has some concrete impacts such as concurrency and the concept of false sharing, which can lead to significantly performance decrease. Let's look at what false sharing is via an example and then see how to prevent it.

```go
type Input struct {
    a int64
    b int64
}

type Result struct {
    sumA int64
    sumB int64
}
```

The goal is to implement a count function that receives a slice of Input and computes the following:

- The sum of all the Input.a fields into Result.sumA
- The sum of all the Input.b fields into Result.sumB

For the sake of the example, we implement a concurrent solution with one goroutine that computes sumA and another that computes sumB:

```go
func count(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	result := Result{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return result
}
```

We spin up two goroutines: one that iterates over each a field and another that iterates over each b field. This example is fine from a concurrency perspective. For instance, it doesn't lead to a data race, because each goroutine increments its own variable. But this example illustrates the false sharing concept that degrades expected performance.

Because sumA and sumB are allocated contiguously, in most cases (seven out of eight), both variables are allocated to the same memory block.

Now, let's assume that the machine contains two cores. In most cases, we should eventually have two threads scheduled on different cores. So if the CPU decides to copy this memory block to a cache line, it is copied twice

![fetch cache](./assets/cache-L1D.png)
*Each block is copied to a cache line on both core 0 and core 1.*

Both cache lines are replicated because L1D (L1 data) is per core. Recall that in our example, each goroutine updates its own variable: sumA on one side, and sumB on the other side.

![copy on cache line](./assets/copy-on-cache-line.png)
*Each goroutine updates its own variable.*

Because these cache lines are replicated, one of the goals of the CPU is to guarantee cache coherency. For example, if one goroutine updates sumA and another reads sumA (after some synchronization), we expect our application to get the latest value.

However, our example doesn't do exactly this. Both goroutines access their own variables, not a shared one. We might expect the CPU to know about this and understand that it isn't a conflict, but this isn't the case. When we write a variable that's in a cache, the granularity tracked by the CPU isn't the variable: it's the cache line. 

When a cache line is shared across multiple cores and at least one goroutine is a writer, the entire cache line is invalidated. This happens even if the updates are logically independent (for example, sumA and sumB). This is the problem of false sharing, and it degrades performance.

**NOTE** Internally, a CPU uses the MESI protocol to guarantee cache coherency. It tracks each cache line, marking it modified, exclusive, shared, or invalid (MESI).

One of the most important aspects to understand about memory and caching is that sharing memory across cores isn't real - it's an illusion. This understanding comes from the fact that we don't consider a machine a black box; instead, we try to have mechanical sympathy with underlying levels.

So how do we solve false sharing? There are two main solutions.

The first solution is to use the same approach we've shown but ensure that sumA and sumB aren't part of the same cache line. For example, we can update the Result struct to add padding between the fields. Padding is a technique to allocate extra memory. Because int64 requries 8-byte allocation and a cache line 64 bytes long, we need 64 - 8 = 56 bytes of paddings

```go
type Input struct {
	a int64
	b int64
}
type Result struct {
	sumA int64
	sumB int64
}
type Result2 struct {
	sumA int64
	_    [56]byte // padding
	sumB int64
}

func count1(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	result := Result{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			result.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return result
}
func count2(inputs []Input) Result {
	wg := sync.WaitGroup{}
	wg.Add(2)
	chA, chB := make(chan int64, 1), make(chan int64, 1)

	go func() {
		defer wg.Done()
		var sumA int64
		for i := 0; i < len(inputs); i++ {
			sumA += inputs[i].a
		}
		chA <- sumA
	}()

	go func() {
		defer wg.Done()
		var sumB int64
		for i := 0; i < len(inputs); i++ {
			sumB += inputs[i].b
		}
		chB <- sumB
	}()

	wg.Wait()
	return Result{
		sumA: <-chA,
		sumB: <-chB,
	}
}

func count3(inputs []Input) Result2 {
	wg := sync.WaitGroup{}
	wg.Add(2)
	rs := Result2{}

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			rs.sumA += inputs[i].a
		}
	}()

	go func() {
		defer wg.Done()
		for i := 0; i < len(inputs); i++ {
			rs.sumB += inputs[i].b
		}
	}()

	wg.Wait()
	return rs
}
```

![memory padding cacheline](./assets/memory-padding-cacheline.png)
*A possible memory allocation. Using padding, sumA and sumB will always be part of different memory blocks and hence different cache lines.*

If we benchmark both solutions (with and without padding), we see that the padding solution is significantly faster (about 40% on my machine). This is an important improvement that results from the addition of padding between the two fields to prevent false sharing.

The second solution is to rework the structure of the algorithm. For example, instead of having both goroutines share the same struct, we can make them communicate their local result via channels. The result benchmark is roughly the same as with padding.

```go
goos: linux
goarch: amd64
pkg: go-learn
cpu: 12th Gen Intel(R) Core(TM) i7-1255U
Benchmark_count1
Benchmark_count1-12          889           1299806 ns/op
Benchmark_count2
Benchmark_count2-12         1130           1063815 ns/op
Benchmark_count3
Benchmark_count3-12         1130           1053121 ns/op
```

In summary, we must remember that sharing memory across goroutines is an illusion at the lowest memory levels. False sharing occurs when a cache line is shared across two cores when at least one goroutine is a writer. If we need to optimize an application that relies on concurrency, we should check whether false sharing applies because this pattern is known to degrade application performance. We can prevent false sharing with either padding or communication.

The following section discusses how CPUs can execute instructions in parallel and how to leverage that capability.

## 93. Not taking into account instruction-level parallelism

Instruction-level parallelism is another factor that can significantly impact performance. Before defining this concept, let's discuss a concrete example and how to optimize it.

We will write a function that receives an array of two int64 elements. This function will iterate a certain number of times (a constant). During each iteration, it will do the following:

- Increment the first element of the array.
- Increment the second element of the array if the first element is even.

Here's the Go version:

```go
func add(s [2]int64) [2]int64 {
	const n = 1_000_000
	for i := 0; i < n; i++ {
		s[0]++
		if s[0]%2 == 0 {
			s[1]++
		}
	}

	return s
}
```

The instructions executed an increment requires both a read and then a write. The sequence of instructions is sequential: first we increment s[0]; then, before incrementing s[1], we need to read s[0] again.

![Three main steps: increment, check, increment](./assets/incre_check_incre.png)
*Three main steps: increment, check, increment*

**NOTE** This sequence of instructions doesn't match the granularity of the assembly instructions. But for clarity throughout this section, we use a simplied view.

Let's take a moment to discuss the theory behind instruction-level parallelism (ILP). A few decades ago, CPU designers stopped focusing solely on clock speed to improve CPU performance. They developed multiple optimizations, including ILP, which allows developers to parallelize the execution of a sequence of instructions. A processor that implements ILP in a single virtual core is called a superscalar process. For example, a CPU executing an application consisting of three instructions, I1, I2 and I3. 

Executing a sequence of instructions requires different stages. In a nutshell, the CPU needs to decode the instructions and execute them. The execution is handled by the execution unit, which performs the various operations and calculations.

The CPU decided to execute the three instructions in parallel. Note that not all the instructions necessarily complete in a single clock cycle. For example, an instruction that reads a value already present in a register will finish in one clock cycle, but an instruction that reads an address that must be fetched from main memory may take dozens of clocks cycles to complete.

![three instructions are execute in parallel](./assets/instructions_parallel.png)

If executed sequentially, this sequence of instructions would have taken the following time (the function t(x) denotes the time the CPU takes to execute instruction x):

total time = t(I1) + t(I2) + t(I3)

Thanks to ILP, the total time is the following:

total time = max(t(I1), t(I2), t(I3))

ILP looks magic, theoretically. But it leads to a few challenges called hazards.

For example, what if I3 sets a variable to 42 but I2 is a conditional instruction (for example, if foo == 1)? In theory, this scenarios should prevent executing I2 and I3 in parallel. This is called a control hazard or branching hazard. In paractice, CPU designers solved control hazards using branch prediction.

TODO: read again

## 94. Not being aware of data alignment

Data alignment is a way to arrange how data is allocated to speed up memory accesses by the CPU. Not being aware of this concept can lead to extra memory consumption and even degraded performance. This section discusses this concept, where it applies, and techniques to prevent under-optimized code.

To understand how data alignment works, let's first discuss what would happen without it. Suppose we allocate two variables, an int32 (32 bits) and a int64 (64 bits)

```go
var i int32
var j int64
```

Without data alignment, on a 64-bit architecture, these two variables could be allocated as shown in figure below.

![data alignment](./assets/data_alignment.png)

The j variable allocation could be spread over two words. If the CPU wanted to read j, it would require two memory accesses instead of one.

To prevent such a case, a variable's memory address should be a multiple of its own size. This is the concept of data alignment. In Go, the alignment guarantees are as follows:

- byte, uint8, int8: 1 byte
- uint16, int16: 2 bytes
- uint32, int32, float32: 4 bytes
- uint64, int64, float64, complex64: 8 bytes
- complex128: 16 bytes

All these types are guaranteed to be aligned: their addresses are a multiple of their size. For example, the address of any int32 variable is a multiple of 4.

Let's get back to the real world. Below figure shows two different cases where i and j are allocated in memory.

![2 ways align](./assets/2-ways-aligned.png)

In the first case, a 32-bit variable was allocated just before i. Therefore, i and j were allocated contiguously. In the second case, the 32-bit variable wasn't allocated before i (for example, it was a 64-bit variable); so, i was allocated at the beginning of a word. To respect data alignment (an address that is a multiple of 64), j can't be allocated alongside i but to the next multiple of 64. The gray box represents 32 bits of padding.

Next, let's look at when padding can be an issue. We will consider the following struct containing three fields:

```go
type Foo struct {
    b1 byte
    i int64
    b2 byte
}
```

We have a byte type (1 byte), an int64 (8 bytes), and another byte type (1 byte). On a 64-bit architecture, the struct is allocated in memory as shown in figure below is allocated first. Because i is an int64, its address must be a multiple of 8. Therefore, it's impossible to allocate it alongside b1 at 0x01. What's the next address that is a multiple of 8? 0x08. b2 is allocated to the next available address that is multiple of 1: 0x10.

![struct occupies 24bytes](./assets/struct_occupies_24bytes.png)

Because a struct's size must be a multiple of the word size (8 bytes), its address isn't 17 bytes but 24 bytes total.

```go
type Foo struct {
	b1 byte
	_  [7]byte // Added by compiler
	i  int64
	b2 byte
	_  [7]byte // Added by the compiler
}
```

Every time a Foo struct is created, it requires 24 bytes in memory, but only 10 bytes contain data - the remaining 14 bytes are padding. Because a struct is an atomic unit, it will never be reorganized, even after a GC; it will always occupy 24 bytes in memory. Note that the compiler doesn't rearrange the fields; it only adds padding to guarantee data alignment.

How can we reduce the amount of memory allocated? The rule of thumb is to reorganized a struct so that its fields are sorted by type size in descending order. In our case, the int64 type is first, followed by the two byte types:

```go
type Foo struct {
    i int64
    b1 byte
    b2 byte
}
```

Figure below shows how this new version of Foo is allocated in memory. i is allocated first and occupies a complete word. The main difference is that now b1 and b2 can live alongside each other in the same word.

![occupies 16 bytes in memory](./assets/data-alignment-optimize.png)

Again, the struct must be a multiple a multiple of the word size; but instead of occupying 24 bytes in memory, it occupies only 16 bytes. We saved 33% of the memory just by moving i to the first position.

What would be concrete impacts if we used the first version of the Foo struct (24bytes) instead of the compacted one? If the Foo structs were retained (for example, an in-memory Foo cache), our application would consume extra memory. But even if the Foo structs weren't retained, there would be other effects. For example, if we created Foo variables frequently, there would be other effects. For example, if we created Foo variables frequently and they were allocated to the heap, the result would be more frequent GCs, impacting overrall application performance.

Speaking of performance, there's another effect on spatial locality. For exmple, let's consider the following sum function that takes a slice of Foo structs as an argument. This function iterates over the slice and sums all the i fields (int64):

```go
func sum(foos []Foo) int64 {
    var s int64
    for i := 0; i < len(foo); i++ {
        s += foos[i].i
    }
    return s
}
```

Because a slice is backed by an array, it means a contiguous allocation of Foo structs. Let's discuss the backing array for two versions of Foo and check two cache lines of data (128 bytes). Each gray bar represents 8 bytes of data, and the darker bars are the i variables (the fields we want to sum). 

![struct to cacheline](./assets/impact-struct-to-cacheline.png)

As we can see, with the latest version of Foo, each cache line is more useful because it contains average 33% more i variables. Therefore, iterating over a Foo slice to sum all the int64 elements is more efficient.

Let's be mindful of data alignment. As we have seen in this section, reorganizing the fields of a Go struct to sort them by size in descending order prevents padding. Preventing padding means allocating more compact struct, possibly leading to optimizations such as reducing the frequency of GCs and better spatial locality.

## 95. Not understanding stack vs. heap
