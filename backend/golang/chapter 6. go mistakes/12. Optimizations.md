# Optimizations

Before we begin this chapter, a disclaimer: in most contexts, writing readable, clear code is better than writing code that is optimized but more complex and difficult to understand. Optimization generally comes with a price, and we advocate that you follow this famous quote from software engineer Wes Dyer:

**Make it correct, make it clear, make it concise, make it fast, in that order.**

## 91. Not understanding CPU caches

You don't have to be an engineer to be a racing driver but you do have to have mechnical sympathy

### CPU architecture

Modern CPUs rely on caching to speed up memory access, in most cases via three caching levels: L1, L2 and L3. On the i5-7300, here are the sizes of these caches:

- L1: 64 KB
- L2: 256 KB
- L3: 4 MB

The i5-7300 has two physical cores but four logical cores (also called virtual cores or threads). In the Intel family, dividing a physical core into multiple logical cores is called Hyper-Threading.

Each physical core (core 0 and core 1) is divided into two logical cores (thread 0 and thread 1). The L1 cache is split into two sub-caches: L1D for data and L1L instructions (each 32 KB). Caching isn't solely related to data - when a CPU executes an application, it can also cache some instructions with the same rationale: to speed up overall execution.

![cpu cache]("./assets/cpu_cache.png")

The physical location of the CPU caches can also explain these differences. L1 and L2 are called on-die, meaning they belong to the same piece of silicon as the rest of processor. Conversely, L3 is off-die, which partly explains the latency differences compared to L1 and L2.

For main memory (or RAM), average accesses are between 50 and 100 times slower than L1. We can access up to 100 variables stored on L1 for the price of a single access to the main memory. Therefore, as Go developers, one avenue for improvement is making sure our applications use CPU caches.

#### Cache line

The concept of cache lines is crucial to understand. But before presenting what they are, let's understand why we need them.

When a specific memory location is accessed (for example, by reading a variable), one of the following is likely to happen in the near future:

- The same location will be referenced again.
- Nearby memory locations will be referenced.

The former refers to temporal locality, and the latter referes to spatial locality. Both are part of a principle called locality of reference.

```go
func sum(s []int64) int64 {
	var total int64
	length := len(s)
	for i := 0; i < length; i++ {
		total += s[i]
	}
	return total
}
```

In this example, temporal locality applies to multiple variables: i, length, and total. Spatial locality applies to code instructions and the slice s. Because a slice is backed by an array allocated contiguously in memory, in this case, access a[0] means also accessing s[1], s[2], and so on.

Temporal locality is part of why we need CPU caches: to speed up repeated accesses to the same variables. However, because of spatial locality, the CPU copies what we call a cache line instead of copying a single variable from the main memory to a cache.

A cache line is a contiguously memory segment of a fixed size, usually 64 bytes (8 int64 variables). Whenever a CPU decides to cache a memory block from RAM, it copies the memory block to a cache line. Because memory is a hierarchy, when the CPU wants to access a specific memory location, it first checks in L1, then L2, then L3, and finally, if the location is not in those caches, in the main memory.

Let's illustrate fetching a memory block with a concrete example. We call the sum function with a slice of 16 int64 elements for the first time. When sum accesses s[0], this memory address isn't in the cache yet. If the CPU decides to cache this variable (we also discuss this decision later in the chapter), it copies the whole memory block. 

![cache line]('./assets/cache_line.png')

Access s[0] makes the CPU copy the 0x000 memory block.

At first, accessing s[0] results in a cache miss because the compulsory miss. However, if the CPU fetches the 0x000 memory block, accessing elements from 1 to 7 results in a cache hit.

**CPU caching strategies**

You may wonder about the exact strategy when a CPU copies a memory block. For example, will it copy a block to all the levels? Only to L1? In this case, what about L1 and L3?

We have to know that different strategies exist. Sometimes caches are inclusive (for example, L2 data is also present in L3), and sometimes caches are exclusive (for example, L3 is called a victim cache because it contains only data evicted from L2).

Let's keep discussing locality of reference and see a concrete example of using spatial locality. 

#### Slice of structs vs struct of slices 

This section looks at an example that compares the execution time of two functions. The first takes as an argument a slice of struct and sums all the a fields:

```go
type Foo struct {
	a int64
	b int64
}

func sumFoo(foos []Foo) int64 {
	var total int64
	for i := 0; i < len(foos); i++ {
		total += foos[i].a
	}

	return total
}

type Bar struct {
	a []int64
	b []int64
}

func sumBar(bar Bar) int64 {
	var total int64
	for i := 0; i < len(bar.a); i++ {
		total += bar.a[i]
	}
	return total
}
```

In the case of sumFoo, we receive a slice of structs containing two fields, a and b. Therefore, we have a succession of a and b in memory. Conversely, in the case of sumBar, we receive a struct containing two slices, a and b. Therefore, all the elements of a are allocated contiguously.

This difference doesn't lead to any memory compaction optimization. But the goal of both functions is to iterate over each a, and doing so requires four cache lines in one case and only two cache lines in the other.

If we benchmark these two functions, sumbar is faster (about 20% on my machine). The main reason is a better spatial locality that makes the CPU fetch fewer cache lines from memory.

This example demonstrates how spatial locality can have a substantial impact on performance. To optimize an application, we should organize data to get the most value out of each individual cache line.

```go
func Benchmark_sumOfSliceStruct(b *testing.B) {
	foos := make([]Foo, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		foos[i] = Foo{
			a: i,
			b: i,
		}
	}

	for i := 0; i < b.N; i++ {
		sumFoo(foos)
	}
}

func Benchmark_sumOfStructSlice(b *testing.B) {
	a := make([]int64, 1000)
	a1 := make([]int64, 1000)
	var i int64
	for i = 0; i < 1000; i++ {
		a[i] = i
		a1[i] = i
	}
	bar := Bar{
		a: a,
		b: a1,
	}

	for i := 0; i < b.N; i++ {
		sumBar(bar)
	}
}
```

However, is using spatial locality enough to help the CPU? We are still missing one crucial characteristic: predictability.

#### Predictability

Predictability refers to the ability of a CPU to anticipate what the application will do to speed up its execution. Let's see a concrete example where a lack of predictability negatively impacts application performance.

```go
type node struct {
	next  *node
	value int64
}

func linkedlist(n *node) int64 {
	var total int64
	for n != nil {
		total += n.value
		n = n.next
	}
	return total
}
```

This function receives a linked list, iterates over it, and increments a total.

On the other side, let's again take the sum2 function that iterates over a slice, one element out of two:

```go
func sum2(s []int64) int64 {
	var total int64
	for i := 0; i < len(s); i++ {
		total += s[i]
	}
	return total
}
```

The two data structures have the same spatial locality, so we may expect a similar execution time for these two functions.

But the function iterating on the slice is significantly faster (about 70% on my machine). What's the reason?

To understand this, we have to discuss the concept of striding. Striding relates to how CPUs work through data.

### Cache placement policy

## 92. Writing concurrent code that leads to false sharing

So far, we have dicussed the fundamental concepts of CPU caching We have seen that some specific caches (typically L1 and L2) aren't shared among all the logical cores but are specific to a physical core. This specificity has some concrete impacts such as concurrency and the concept of false sharing, which can lead to significantly performance decrease.

```go
type Input struct {
    a int64
    b int64
}

type Result struct {
    sumA int64
    sumB int64
}
```
