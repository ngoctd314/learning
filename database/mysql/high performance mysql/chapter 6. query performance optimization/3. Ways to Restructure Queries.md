# Ways to Restructure Queries

## Complex Queries Versus Many Queries 

One important query design question is whether is't preferable to breakup a complex query into several simpler queries. The traditional approach to database design emphasizes doing as much work as possible with as few queries as possible. This approach was historically better because of the cost of network communication and the overhead of the query parsing and optimization stages.

However, this advice doesn't applying as much to MySQL, because it was designed to handle connecting and disconnecting very efficiently and to respond to small and simple queries very quickly. All else being equal, it's still a good idea to use as few queries as possible, but sometimes you can make a query more efficient by decomposing it and executing a few simple queries instead of one complex one. Don't be afraid to do this.

### Chopping Up a Query

Another way to slice up a query is to divide and conquer, keeping it essentially the same but running it in smaller "chunks" that affect fewer rows each time.

Purging old data is a grate example. Periodic purge jobs might need to remove quite a bit of data, and doing this in one massive query could lock a lot of rows for a long time, fill up transaction logs, hog resources, and block small quires that shouldn't be interrupted. Chopping up the DELETE statement and using medium-size the queries can improve performance considerably, and reduce replication lag when a query is replicated. 

```sql
DELETE FROM tbl WHERE create_at < DATE_SUB(NOW(), INTERNAL 3 MONTH);
```

You could do something like the following pseudocode:

```txt
rows_affected = 0
do {
    rows_affected = do_query(
        "DELETE FROM messages WHERE created < DATE_SUB(NOW(), INTERNAL 3 MONTH) LIMIT 10000"
    )
} while rows_affected > 0
```

Deleteing 10,000 rows at a time is typically a large enough task to make each query efficient, and a short enough task to minimize the impact on the server (transaction storage engines might benefit from smaller transactions). It might also be a good idea to add some sleep time between the DELETE statements to spread the load over time and reduce the amount of time locks are held.

## Join Decomposition

Many high-performance applications use join decomposition. You can decomposition a join by running multiple single-table queries instead of a multitable join, and then performing the join in the application.

Instead of this single query:

```sql
SELECT * FROM tag
    JOIN tag_post ON tag_post.tag_id=tag.id
    JOIN post ON tag_post.post_id=post.id
WHERE tag.tag='mysql';
```

You might run these queries

```sql
SELECT * FROM tag WHERE tag='mysql';
SELECT * FROM tag_post WHERE tag_id = 1234;
SELECT * FROM post WHERE post.id in (123,234,345);
```

Such re-structuring can actually significant performance advantages:

- Caching can be more efficient. Many applications cache objects that map directly to tables. In this example, if the object with the tag mysql is already cached, the application can skip the first query. If you find posts with an IN of 123 or 234 in the cache, you can remove them from the IN() list. The query cache might also benifit from this strategy. If only one of the tables changes frequently, decomposing a join can reduce the number of cache invalidations. 
- Executing the queries individually can sometimes reduce lock contention.
- Doing joins in the application makes it easier to scale the database by placing tables on different servers.
- The queries themselves can be more efficient. Using an IN() list instead of a join lets MySQL sort row IDs and retrieve rows more optimally than might be possible with a join.
- You can reduce redundant row accesses. Doing a join in the application means you retrieve each row only once, whereas a join in the query is essentially a denormalization that might repreatedly access the same data. For the same reason, such re-structuring might also reduce the total network traffic and memory usage.
- To some extent, you can view this technique as manually implementing a hash join instead of the nested loops algorithm.

As a result, doing joins in the application can be more efficient when you cache and reuse a lot of data from earlier queries, you distribute data across multiple servers...

