# Show query basics: Optimize Data Access

The most basic reason a query doesn't perform well is because it's working with too much data. Some queries just have to sift through a lot of data and can't be helped. Most bad queries can be changed to access less data. We've found it useful to analyze a poorly performing query in two steps:

1. Find out whether your application is retrieving more data than you need. That usually means it's accessing too many rows, but it might also be accessing too many columns.
2. Find out whether the MySQL server is analyzing more rows than it needs.

## Are you asking the database for data you don't need

Some queries ask for more data than they need and then throw some of it away. This demands extra work of the MySQL server, adds network overhead, and consumes memory and CPU resources on the application server.

**Network overhead is worst if the application is on a different host from the server, but transferring data between MySQL and the application isn't free event if they're on the same server.**

**- Fetching more rows than needed**

One common mistake is assuming that MySQl provides results on demand, rather than calculating and returning the full result set. These developers are used to techniques such as issuing a SELECT statement that returns many rows, then fetching the first N rows and closing the result set (e.g. fetching the 100 most recent articles for a news site when they only need to show 10 of them on the front page). They think MySQL will provide them with these 10 rows and stop executing the query, but what MySQL really does is generate these 10 rows and stop executing the query, but what MySQL really does is generate the complete result set. The client lib then fetches all the data and discards most of it. The best solution is to add a LIMIT clause to the query.

Bad query

```go
rows, _ := db.Query("SELECT * FROM tbl") // query all rows from database
i := 0
for rows.Next() {
    i++
    if i == 10 {
        rows.Close()
    }
    break
    // scan data here
}
```

Good query

```go
rows, _ := db.Query("SELECT * FROM tbl LIMIT 10")
defer rows.Close()
for rows.Next() {
    // scan data here
}
```

**- Fetching all columns**

You should always be suspicious when you see `SELECT *`. Do you really need all columns? Probably not. Retrieving all columns can prevent optimizations such as covering indexes, as well as adding I/O, memory, and CPU overhead for the server.

Some DBAs ban `SELECT *` universally because of this fact, and to reduce the risk of problems when someone alters the table's column list.

Of course, asking for more data than you really need is not always bad. In many case we've investigated, people tell us the wasteful approach simplifies development, because it lets the developer use the same bit of code in more than one place. That's a reasonable consideration, as long as you know what it costs in terms of performance. It might also be useful to retrieve more data than you actually need if you use some type of caching in your application, or if you have another benefit in mind. Fetching and caching full objects might be preferable to running many separate queries that retrieve only parts of the object.

**- Fetching all columns from a multi-table join**

If you want to retrieve all actors who appear in the film Academy Dinosaur, don't write the query this way:

```sql
SELECT * FROM sakila.actor
INNER JOIN sakila.film_actor USING (actor_id)
INNER JOIN sakila.film USING (film_id)
WHERE sakila.film.title = "Academy Dinosaur";
```

That returns all columns from all three tables. Instead, write the query as follows:

```sql
SELECT sakila.actor.* FROM sakila.actor...;
```

**- Fetching the same data repeatedly**

If you're not careful, it's quite easy to write application code that retrieves the same data repeatedly from the database server, executing the same query to fetch it. For example, if you want to find out a user's profile image URL to display next to a list of comments, you might request this repeatedly for each comment. Or you could cache it in the first time fetch it, and reuse it thereafter. The later approach is much more efficient.

## Is MySQL examining Too much data

In MySQL the simplest query cost metrics are:

- Response time
- Number of rows examined
- Number of rows returned

None of these metrics is a perfect way to measure query costs, but they reflect roughly how much data MySQL must access internally to execute a query and translate approximately into how fast the query runs. All these metrics are logged in the slow query log, so looking at the slow query log is one of the best ways to find queries that examine too much data.

Looking at the slow query log is one of the best ways to find queries that examine too much data.

### Response time

Response time if the sum of two things: service time and queue time. Service time is how long it takes the server to actually process the query. Queue time is the the portion of response time during which server isn't really executing the query - it's waiting for something such as waiting for an I/O operation to complete, waiting for a row lock, and so forth. The problem is, you can't break the response time down into these components unless you can measure them individually, which is usually hard to do. In general, the most common and important waits you'll encouter are I/O and lock waits, but you shouldn't count on that, because it varies a lot.

As a result, response time is not consistent under varying load conditions. Other factors - such as storage engine locks (table locks and row locks), high concurrency, and hardware - can also have a considerable impact on response times. Response time can also be both a symptom and a cause of problems, and it's not always obvious which is the case.

### Rows examined and rows returned

It's useful to think about the number of rows examined when analyzing queries, because you can see how efficiently the queries are finding the data you need.

However, this is not a perfect metric for finding "bad" queries. Not all row accesses are equal. Shorter rows are faster to access, and fetching rows from memory is much faster than reading them from disk.

Ideally, the number of rows examined would be same as the number returned, but in practice this is rarely possible. For example, when constructing rows with joins, the server must access multiple rows to generate each row in the result set.

### Rows examined and access types

When you're thinking about the cost of query, consider the cost of finding a single row in a table. MySQL can be use several access methods to find and return a row. Some require examining many rows, but others might be able to generate the result without examining any.

The access method(s) appear in the type column in EXPLAIN's output. The access types range from a full table scan to index scans, range scans, unique index lookups, and constants. Each of these is faster than the one before it, because it requires reading less data. You don't need to memorize the access types, but you should understand the general concepts of scanning a table, scanning an index, range accesses, and single value access.

If you aren't getting a good access type, the best way to solve the problem is usually by adding an appropriate index. Indexes let MySQL find rows with more efficient access type that examines less data.

```sql
SELECT * FROM tbl where id = 1;
```

This query will return 10 rows, and EXPLAIN shows that MySQL uses the ref access type on the idx_fk_film_id index to execute the query:

```sql
EXPLAIN SELECT * FROM film_actor WHERE film_id = 1;

id: 1
select_type: SIMPLE
type: ref
possible_keys: idx_fk_film_id
key: idx_fk_film_id
key_len: 2
ref: const
rows: 10
Extra:
```

EXPLAIN shows that MySQL estimated it needed to access only 10 rows. In other words, the query optimizer knew the chosen access type could satisfy the query efficiently. What would happen if there were no suitable index for the query? MySQL would have to use a less optimal access type, as we can see if we drop the index and run the query again:

```sql
ALTER TABLE film_actor DROP FOREIGN KEY fk_film_actor_film;
ALTER TABLE film_actor DROP KEY idx_film_id;
EXPLAIN SELECT * FROM film_actor WHERE film_id = 1;

id: 1
select_type: SIMPLE
possible_keys: NULL
type: ALL
key: NULL
key_len: NULL
ref: NULL
rows: 5000
Extra: using where
```

Predictably, the access type has changed to a full table scan (ALL), and MySQL now estimates it'll have to examine 5.073 rows to satisfy the query. The "Using where" in the Extra column shows that the MySQL server is using WHERE clause to discard rows after the storage engine reads them. 

In general, MySQL can apply a `WHERE` clause in three ways, from best to worst:

- Apply the conditions to the index lookup operation to eliminate non-matching rows. This happens at the storage engine layer.
- Use a covering index ("Using index" in the Extra column) to avoid row accesses, and filter out non-matching rows after retrieving each result from the index. This happens at the server layer, but it doesn't require reading rows from the table.
- Retrieve rows from the table, then filter non-matching rows ("Using where" in the Extra column). This happens at the server layer and requires the server to read rows from the table before it can filter them. 

This example illustrates how important it is to have good indexes

You don't need to memorize the access types, but you should understand the general concepts of scanning a table, scanning an index, range access, and single value access.

In general MySQL can apply a WHERE clause in three ways, from best to worst:

- Apply the conditions to the index lookup operation to eliminate non-matching rows. This happens at the storage engine layer.
- Use a covering index to avoid row accesses, and filter out non-matching rows after retrieving each result from the index.
- Retrieve rows from the table, then filter non-matching rows. ("Using where" in the Extra column). This happens at the server layer and requires the server to read rows from the table before it can fitler them.

```sql
SELECT actor_id, COUNT(*) FROM film_actor GROUP BY actor_id;
```

This query returns only 200 rows, but it need thousands of rows to build the result set. An index can't reduce the number of rows examined for a query like this one.

Unfortunately, MySQL does not tell you how many of the rows it accessed were used to build the result set; it tells only the total number of rows it accessed. Many of these rows could be eliminated by a WHERE clause and end up not contributing to the result set.

If you find a huge number of rows were examined to produce relatively few rows in the result, you can try some more sophisticated fixes:

- Use covering indexes, which store data so that the storage engine doesn't have to retrieve the complete rows.
- Change the schema. An example is using summary tables
- Rewrite a complicated query so the MySQL optimizer is able to execute it optimally.
