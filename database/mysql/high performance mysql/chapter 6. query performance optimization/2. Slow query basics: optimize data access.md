# Show query basics: Optimize Data Access

The most basic reason a query doesn't perform well is because it's working with too much data. Some queries just have to sift through a lot of data and can't be helped. Most bad queries can be changed to access less data. We've found it useful to analyze a poorly performing query in two steps:

1. Find out whether your application is retrieving more data than you need. That usually means it's accessing too many rows, but it might also be accessing too many columns.
2. Find out whether the MySQL server is analyzing more rows than it needs.

## Are you asking the database for data you don't need

Some queries ask for more data than they need and then throw some of it away. This demands extra work of the MySQL server, adds network overhead, and consumes memory and CPU resources on the application server.

**Network overhead is worst if the application is on a different host from the server, but transferring data between MySQL and the application isn't free event if they're on the same server.**

- Fetching more rows than needed

One common mistake is assuming that MySQl provides results on demand, rather than calculating and returning the full result set. These developers are used to techniques such as issuing a SELECT statement that returns many rows, then fetching the first N rows and closing the result set (e.g. fetching the 100 most recent articles for a news site when they only need to show 10 of them on the front page). They think MySQL will provide them with these 10 rows and stop executing the query, but what MySQL really does is generate these 10 rows and stop executing the query, but what MySQL really does is generate the complete result set. The client lib then fetches all the data and discards most of it. The best solution is to add a LIMIT clause to the query.

Bad query

```go
rows, _ := db.Query("SELECT * FROM tbl")
i := 0
for rows.Next() {
    i++
    if i == 10 {
        rows.Close()
    }
    break
    // scan data here
}
```

Good query

```go
rows, _ := db.Query("SELECT * FROM tbl LIMIT 10")
defer rows.Close()
for rows.Next() {
    // scan data here
}
```

- Fetching all columns from a multi-table join

If you want to retrieve all actors who appear in the film Academy Dinosaur, don't write the query this way:

```sql
SELECT * FROM sakila.actor
INNER JOIN sakila.film_actor USING (actor_id)
INNER JOIN sakila.film USING (film_id)
WHERE sakila.film.title = "Academy Dinosaur";
```

That returns all columns from all three tables. Instead, write the query as follows:

```sql
SELECT sakila.actor.* FROM sakila.actor...;
```

- Fetching all columns

**You should always be suspicious when you see SELECT \*.** Do you really need all columns? Probably not. Retrieving all columns can prevent optimizations such as covering indexes, as well as adding I/O, memory, and CPU overhead for the server.

Some DBAs ban SELECT \* universally because of this fact, and to reduce the risk of problems when someone alters the table's column list.

- Fetching the same data repeatedly

If you're not careful, it's quite easy to write application code that retrieves the same data repeatedly from the database server, executing the same query to fetch it. For example, if you want to find out a user's profile image URL to display next to a list of comments, you might request this repeatedly for each comment. Or you could cache it in the first time fetch it, and reuse it thereafter. The later approach is much more efficient.

## Is MySQL examining Too much data

In MySQL the simplest query cost metrics are:

- Response time
- Number of rows examined
- Number of rows returned

Looking at the slow query log is one of the best ways to find queries that examine too much data.

### Response time

Response time if the sum of two things: service time and queue time. Service time is how long it takes the server to actually process the query. Queue time is the the portion of response time during which server isn't really executing the query - it's waiting for something such as waiting for an I/O operation to complete, waiting for a row lock, and so forth.

As a result, response time is not consistent under varying load conditions. Other factors - such as storage engine locks (table locks and row locks), high concurrency, and hardware - can also have a considerable impact on response times. Response time can also be both a symptom and a cause of problems, and it's not always obvious which is the case.

### Rows examined and rows returned

It's useful to think about the number of rows examined when analyzing queries, because you can see how efficiently the queries are finding the data you need.

However, this is not a perfect metric for finding "bad" queries. Not all row accesses are equal. Shorter rows are faster to access, and fetching rows from memory is much faster than reading them from disk.

Ideally, the number of rows examined would be same as the number returned, but in practice this is rarely possible. For example, when constructing rows with joins, the server must access multiple rows to generate each row in the result set.

### Rows examined and access types

When you're thinking about the cost of query, consider the cost of finding a single row in a table. MySQL can be use several access methods to find and return a row. Some require examining many rows, but others might be able to generate the result without examining any.

The access method(s) appear in the type column in EXPLAIN's output. The access types range from a full table scan to index scans, range scans, unique index lookups, and constants. Each of these is faster than the one before it, because it requires reading less data. You don't need to memorize the access types, but you should understand the general concepts of scanning a table, scanning an index, range accesses, and single value access.

If you aren't getting a good access type, the best way to solve the problem is usually by adding an appropriate index. Indexes let MySQL find rows with more efficient access type that examines less data.

```sql
SELECT * FROM tbl where id = 1;
```

You don't need to memorize the access types, but you should understand the general concepts of scanning a table, scanning an index, range access, and single value access.

In general MySQL can apply a WHERE clause in three ways, from best to worst:

- Apply the conditions to the index lookup operation to eliminate non-matching rows. This happens at the storage engine layer.
- Use a covering index to avoid row accesses, and filter out non-matching rows after retrieving each result from the index.
- Retrieve rows from the table, then filter non-matching rows. ("Using where" in the Extra column). This happens at the server layer and requires the server to read rows from the table before it can fitler them.

```sql
SELECT actor_id, COUNT(*) FROM film_actor GROUP BY actor_id;
```

This query returns only 200 rows, but it need thousands of rows to build the result set. An index can't reduce the number of rows examined for a query like this one.

Unfortunately, MySQL does not tell you how many of the rows it accessed were used to build the result set; it tells only the total number of rows it accessed. Many of these rows could be eliminated by a WHERE clause and end up not contributing to the result set.

If you find a huge number of rows were examined to produce relatively few rows in the result, you can try some more sophisticated fixes:

- Use covering indexes, which store data so that the storage engine doesn't have to retrieve the complete rows.
- Change the schema. An example is using summary tables
- Rewrite a complicated query so the MySQL optimizer is able to execute it optimally.
