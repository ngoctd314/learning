# Cache and Summary Tables

The terms "cache table" and "summary table" don't have standardized meanings. We use the term "cache tables" to refer to tables that contain data that can be easily, if more slowly, retrieved from the schema. When we say "summary tables" we mean tables that hold aggregated data from GROUP BY queries (data that is not logically redundant). Some people also use the term "roll-up tables" for these tables.

Staying with the website example, suppose you need to count the number of messages posted during the previous 24 hours. It would be impossible to maintain an accurate real-time counter on a busy site. Instead, you could generate a summary table ever hour. You can often do this with a single query, and it's more efficient than maintaining counters in the real time. The drawback is that the counts are not 100% accurate. 

If you need to get an accurate count of messages posted during the previous 24-hour period (with no staleness), there is another option. Begin with a per-hour summary table. You can then count the exact number of messages posted in a given 24-hour period by adding the number of messages in the 23 whole hours contained in that period, the partial hour at the beginning of the period, and the partial hour at the end of the period.

```sql
CREATE TABLE msg_per_hr (
    hr DATETIME NOT NULL,
    cnt INT UNSIGNED NOT NULL,
    PRIMARY KEY(hr)
);
```
Either approach - an inexact count or an exact count with small range queries to fill in the gaps is more efficient than counting all the rows in the message table. This is the key reason for creating summary tables. These statistics are expensive to compute in real time, because they require scanning a lot of data, of queries that will only run efficiently with special indexes that you don't want to add because of the impact they will have on updates. Computing the most active users or the most frequent "tags" are typical examples of such operations.

When using cache and summary tables, you have to decide whether to maintain their data in real time or with periodic rebuilds. Which is better will depend on your application, but a periodic rebuilds. Which is better will depend on your application.

When you rebuild summary and cache tables, you'll often need their data to remain available during the operation. YOu can achieve this by using a shadow table, which is a table you build "behind" the real table. When you're done building it, you can swap the tables with an atomic rename. For example, if you need to rebuild my_summary, you can create my_summary_new, fill it with data, and swap it with real table.

```sql
DROP TABLE IF EXISTS my_summary_new, my_summary_old;
CREATE TABLE my_summary_new LIKE my_summary;
-- populate my_summary_new as desired
RENAME TABLE my_summary TO my_summary_old, my_summary_new TO my_summary;
```

If you rename the original my_summary table my_summary_old before assigning the name my_summary to the newly rebuilt table, as we've done here, you can keep the old version until you're ready to overwrite it at the next rebuild. It's handy to have it for a quick rollback if the new table has a problem.

## Counter Tables

An application that keeps counts in a table can run into concurrency problems when updating the counters. Such tables are very common in web applications. You can use them to cache the number of friends a user has, the number of downloads of a file, and so on. It's often a good idea to build a separate table for the counters, to keep it small and fast. 

To keep things as simple as possible, you have a counter table with a single row that just counts hits on your website:

```sql
CREATE TABLE hit_counter (
    cnt INT UNSIGNED NOT NULL
) ENGINE=InnoDB;
```
Each hti on the website updates the counter:

```sql
UPDATE hit_counter SET cnt = cnt + 1;
```

The problem is that this single row effectively a global "mutex" for any transaction that updates the counter. It will serialize those transactions. You can get higher concurrency by keeping more than one row and updating a random row. This requires the following change to the table:

```sql
CREATE TABLE hit_counter (
    slot TINYINT UNSIGNED NOT NULL PRIMARY KEY,
    cnt INT UNSIGNED NOT NULL 
) ENGINE=InnoDB;
```
Pre-populate the table by adding 100 rows to it. Now the query can just choose a random slot and update it:

```sql
UPDATE hit_counter SET cnt = cnt + 1 WHERE slot = RAND() * 100;
```
To retrieve statistics, just use aggregate queries:

```sql
SELECT SUM(cnt) FROM hit_counter;
```

A common requirement is to start new counters every so often (for example, once a day). If you need to do this, you can change the schema slightly:

```sql
CREATE TABLE daily_hit_counter (
    day DATE NOT NULL,
    slot TINYINT UNSIGNED NOT NULL,
    cnt INT UNSIGNED NOT NULL,
    PRIMARY KEY (day, slot)
) ENGINE=InnoDB;
```

You don't want to pregenerate rows for this scenario. Instead, you can use ON DUPLICATE KEY UPDATE:

```sql
INSERT INTO daily_hit_counter(day, slot, cnt)
    VALUES(CURRENT_DATE, RAND() * 100, 1)
    ON DUPLICATE KEY UPDATE cnt = cnt + 1;
```

**Faster Reads, Slower Writes**
You'll often read extra indexes, redundant fields, or even cache and summary tables to speed up read queries. These add work to write queries and maintenance jobs, but this is still a technique you'll see a lot of when you design for high performance: you amortize the cost of the slower writes by speeding up reads significantly.
