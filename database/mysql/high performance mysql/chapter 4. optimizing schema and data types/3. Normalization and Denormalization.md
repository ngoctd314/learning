# Normalization and De-normalization

There are usually many ways to represent any given data, ranging from fully normalized to fully de-normalized and anything in between. In a normalized database, each fact is represented once and only once. Conversely, in a de-normalized database, information is duplicated, or stored in multiple places.

Example about de-normalized database

|EMPLOYEE|DEPARTMENT||
|-|-|-|
|Jones|Accounting|Jones|
|Smith|Engineering|Smith|
|Brown|Accounting|Jones|
|Green|Engineering|Smith|

The problem with this schema is that inconsistencies can occur while the data is being modified. Say Brown takes over as the head of the Accounting department. We need to update multiple rows to reflect this change, and that's a pain and introduces opportunities for error. To avoid these problems, we need to normalize the table by separating the employee and department entities. This process results in the following two tables for employees.

## Pros and Cons of a Normalized Schema

People who ask for help with performance issues are frequently advised to normalize their schemas, especially if the workload is write-heavy. This is often good advice. It works well for the following reasons:

- Normalized updates are usually faster than de-normalized updates.
- When the data is well normalized, there's little or no duplicated data, so there's less data to change.  
- Normalized tables are usually smaller, so they fit better in memory and perform better.
- The lack of redundant data means there's less need for DISTINCT or GROUP BY queries when retrieving lists of values.

The drawbacks of a normalized schema usually have to do with retrieval. Any nontrivial query on a well-normalized schema will probably require at least one join, and perhaps several. This is not only expensive, but it can make some indexing strategies impossible.

## Pros and Cons of a De-normalized  Schema

A de-normalized schema works well because everything is in the same table, which avoids joins.

If you don't need to join tables, the worst case for most queries - even the ones that don't use indexes is a full table scan. This can be much faster than a join when the data doesn't fit in memory, because it avoids random I/O.

A single table can also allow more efficient index strategies.

## A Mixture of Normalized and Denormalized
