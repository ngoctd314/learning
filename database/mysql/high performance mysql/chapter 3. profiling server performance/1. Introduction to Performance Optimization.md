# Introduction to Performance Optimization

Ask 10 people to define performance and you'll probably get 10 different answers, filled with terms such as "queries per second", "CPU utilization", "scalability", and so on. This is file for most purposes, because people understand performance differently in different contexts, but we will use a formal definition. **Performance is measured by the time required to complete a task.** In other words, performance is response time. This is a very important principle. We measure performance by tasks and time, not by resources. A database server's purpose is to execute SQL statements, so the tasks we care about are queries or statements - the bread-and-butter SELECT, UPDATE, INSERT, and so on. A database server's performance is measured by query response time, and the unit of measurement is time per query.

Performance optimization is the practice of reducing response time as much as possible for a given workload.

We find that many people are very confused about this. If you think performance optimization requires you to reduce CPU utilization, for example, you're thinking about reducing resource consumption. But this is a trap. Resources are the to be consumed. Sometimes making things faster requires that you increase resource consumption. Sometimes making things faster requires that you increase resource consumption. We've upgraded many times from an old version of MySQL with an ancient version of InnoDB, and witnessed a dramatic increase in CPU utilization as a result. Looking at query response time doing useful work and less time fighting with itself. Looking at query response time is the best way to know whether the upgrade was an improvement.  Sometimes as upgrade introduces a bug such as not using an index, which can also manifest as increased CPU utilization.

Similarly, if you thought that performance optimization was about improving queries per second, then you were thinking about throughput optimization. Increased throughput can be considered as a side effect of performance optimization. Optimization queries makes it possible for the server to execute more queries per second, because each one requires less time to execute when the server is optimized. (The unit of throughput is queries per time, which is the inverse of our definition of performance.)

So if the goal is to reduce response time, we need to understand why the server requires a certain amount of time to respond to a query, and reduce or eliminate whatever unnessary work it's doing to achieve the result. In other words, we need to measure where the time goes. This leads to our second important principle of optimization: you cannot reliably optimize what you cannot measure. Your first job is therefore to measure when time is spent.

How do you determine which tasks to target for optimization? This is why profiling was invented.

**How Do You Know If Measurements Are Right?**

If measurements are so important, then what if the measurements are wrong? In fact, measurements are always wrong. The measurement of a quantity is not the same as the quantity itself. The measurements might not be wrong enough to make a big difference, but they're wrong. So the question really should be, "How uncertain is the measurement?" This is a topic that's addressed in great detail in other books, so we won't tackle it here. Just be conscious that you're working with measurements, not the actual quantities they represent. As usual, the measurements can be presented in confusing or ambiguous ways, which can lead to wrong conclusions. 

## Optimization Through Profiling

Profiling is the primary means of measuring and analyzing where time is consumed. Profiling entails two steps: measuring tasks and time elapsed, and aggregating and storing the results so that the important tasks bubble to the top.

Profiling tools all work in pretty much the same way. When a task begins, they start a timer, and when it ends, they stop the timer and subtract the start time from the end 

We will actually discuss two kinds of profiling: execution-time profiling and wait analysis. Execution-time profiling shows which tasks consume the most time, whereas wait analysis shows where tasks gets stuck or blocked the most.

When tasks are slow because they're consuming too many resources and are spending most of their time executing, they won't spend much time waiting, and wait analysis will not be useful. The reverse is true, too: when tasks are waiting all the time and not consuming any resources, measuring where they spend time executing won't be very helpful. If you're not sure which kind of time consumption is the problem, you might need to do both.

We've observed that many people, when trying to optimize something, spend the bulk of their time changing things and very little time measuring. In contrast, we aim to spend most of our time - perhaps upwards of 90% - measuring where the response time is spent. If we don't find the answer, we might not have measured correctly or completely.

We will actually discuss two kinds of profiling: execution-time profiling and wait analysis. Execution-time profiling shows which tasks consume the most time, whereas wait analysis shows where tasks get stuck or blocked the most.

When tasks are slow because they're consuming too many resources and are spending most of their time executing, they won't spend much time waiting, and wait analysis will not be useful. The reverse is true, too: when tasks are waiting all the time and not consuming any resources, measuring where they spend time executing
