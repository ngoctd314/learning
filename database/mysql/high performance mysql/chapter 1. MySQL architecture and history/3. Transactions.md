# Transactions

A transaction is a group of SQL queries that are treated atomically, as a single unit of work. If the database engine can apply the entire group of queries to a database, it does so, but if any of them can't be done because of a crash or other reason, none of them is applied. It's all or nothing.

```sql
START TRANSACTION;
SELECT balance FROM checking WHERE customer_id = 1;
UPDATE checking SET balance = balance - 200 WHERE customer_id = 1;
UPDATE savings SET balance = balance + 200 WHERE customer_id = 1;
COMMIT;
```

Transaction aren't enough unless the system passes the ACID test. ACID stands for Atomicity, Consistency, Isolation and Durability.

**Atomicity**
A transaction must function as a single indivisible unit of work so that the entire transaction is either applied or rolled back. When transactions are atomic, there is no such thing as a partially completed transaction: it's all or nothing.

**Consistency**
The database should always move from one consistent state to the next. When the transaction is never committed, none of the transaction's changes are ever reflected in the database.

**Isolation**
The results of a transaction are usually invisible to other transactions until the transaction is complete. When we discuss isolation levels, you'll understand why we said usually invisible.

**Durability**
Once committed, a transaction's changes are permanent. This means the changes must be recorded such that data won't be lost in a system crash.

ACID transactions ensure that banks don't lose your money. It is generally extremely difficult or impossible to do this with application logic.

Just as with increased lock granularity, the downside of this extra security is that the database server has to do more work. A database server with ACID transactions also generally requires more CPU power, memory, and disk space than one without them. You can decide whether your application needs transaction. If you don't really need them, you might be able to get higher performance with a non-transactional storage engine for some kinds of queries. You might be able to use LOCK TABLES to give the level of protection you need without transactions.

## Isolation levels

Isolation is more complex than it looks. The SQL standard defines four isolation levels, with specific rules for which changes are and aren't visible inside and outside a transaction. Lower isolation levels typically allow higher concurrency and have lower overhead.

Each storage engine implements isolation levels slightly differently, and they don't necessarily match what you might expect if you're used to another database product. You should read the manuals for whichever storage engines you decide to use.

**READ UNCOMMITTED (dirty read)**

In the READ UNCOMMITTED isolation level, transactions can view the results of uncommitted transactions. At this level, many problems can occur unless you really, really known what you are doing and have a good reason for doing it. This level is rarely used in practice, because it performance isn't much better than the other levels, which have many advantages. Reading uncommitted data is also known as a dirty read.

In this example, process2 show created_at from process1 (all process committed sucessfully)
```go
var (
	conn *sql.DB
	ctx  = context.Background()
)

func init() {
	var err error
	conn, err = sql.Open("mysql", "root:secret@(192.168.49.2:30300)/learn_lock?parseTime=true")
	if err != nil {
		log.Fatal(err)
	}
}

func main() {
	guard1, guard2 := make(chan struct{}, 1), make(chan struct{}, 1)

	releaseProcess1 := func() {
		guard1 <- struct{}{}
	}
	blockProcess1 := func() {
		<-guard1
	}
	releaseProcess2 := func() {
		guard2 <- struct{}{}
	}
	blockProcess2 := func() {
		<-guard2
	}

	var wg sync.WaitGroup
	wg.Add(2)

	// process 1
	go func() {
		defer wg.Done()
		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadUncommitted,
		})
		rs, err := tx.Exec("INSERT INTO tbl (created_at) VALUES (?)", time.Now())
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		releaseProcess2()

		fmt.Println(rs.LastInsertId())

		blockProcess1()
		tx.Commit()
	}()

	// process 2
	go func() {
		defer wg.Done()
		blockProcess2()

		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadUncommitted,
		})
		rows, err := tx.Query("SELECT created_at FROM tbl")
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		for rows.Next() {
			var dest time.Time
			if err := rows.Scan(&dest); err != nil {
				log.Println(err)
			}
			fmt.Println("dest", dest)
		}

		tx.Commit()
		releaseProcess1()
	}()

	wg.Wait()
}
```

In this example, process2 still show dest ..., but in final phase, we don't see any output

```go
var (
	conn *sql.DB
	ctx  = context.Background()
)

func init() {
	var err error
	conn, err = sql.Open("mysql", "root:secret@(192.168.49.2:30300)/learn_lock?parseTime=true")
	if err != nil {
		log.Fatal(err)
	}
}

func main() {
	guard1, guard2 := make(chan struct{}, 1), make(chan struct{}, 1)

	releaseProcess1 := func() {
		guard1 <- struct{}{}
	}
	blockProcess1 := func() {
		<-guard1
	}
	releaseProcess2 := func() {
		guard2 <- struct{}{}
	}
	blockProcess2 := func() {
		<-guard2
	}

	var wg sync.WaitGroup
	wg.Add(2)

	// process 1
	go func() {
		defer wg.Done()
		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadUncommitted,
		})
		rs, err := tx.Exec("INSERT INTO tbl (created_at) VALUES (?)", time.Now())
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		releaseProcess2()

		fmt.Println(rs.LastInsertId())

		blockProcess1()
		tx.Rollback()
		// tx.Commit()
	}()

	// process 2
	go func() {
		defer wg.Done()
		blockProcess2()

		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadUncommitted,
		})
		rows, err := tx.Query("SELECT created_at FROM tbl")
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		for rows.Next() {
			var dest time.Time
			if err := rows.Scan(&dest); err != nil {
				log.Println(err)
			}
			fmt.Println("dest", dest)
		}

		tx.Commit()
		releaseProcess1()
	}()

	wg.Wait()

	rows, err := conn.Query("SELECT created_at FROM tbl")
	if err != nil {
		log.Println(err)
		return
	}
	for rows.Next() {
		var dest time.Time
		if err := rows.Scan(&dest); err != nil {
			log.Println(err)
		}
		fmt.Println("final", dest)
	}
}
```

**READ COMMITTED (non-repeatable read)**

The default isolation level for most database systems (but not MySQL) is READ COMMITTED. It satisfied the simple definition of isolation used earlier: a transaction will see only those changes made by transactions that were already committed when it began, and its changes won't be visible to others until is has committed. This level is still allows what's known as non-repeatable read???. This means you can run the same statement twice and see the different data.

```go
var (
	conn *sql.DB
	ctx  = context.Background()
)

func init() {
	var err error
	conn, err = sql.Open("mysql", "root:secret@(192.168.49.2:30300)/learn_lock?parseTime=true")
	if err != nil {
		log.Fatal(err)
	}
}

func main() {
	guard1, guard2 := make(chan struct{}, 1), make(chan struct{}, 1)

	releaseProcess1 := func() {
		guard1 <- struct{}{}
	}
	blockProcess1 := func() {
		<-guard1
	}
	releaseProcess2 := func() {
		guard2 <- struct{}{}
	}
	blockProcess2 := func() {
		<-guard2
	}

	var wg sync.WaitGroup
	wg.Add(2)

	// process 1
	go func() {
		blockProcess1()

		defer wg.Done()
		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadCommitted,
		})
		_, err := tx.Exec("INSERT INTO tbl (created_at) VALUES (?)", time.Now())
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		tx.Commit()

		releaseProcess2()
	}()

	// process 2
	go func() {
		defer wg.Done()
		tx, _ := conn.BeginTx(ctx, &sql.TxOptions{
			Isolation: sql.LevelReadCommitted,
		})
		rows, err := tx.Query("SELECT created_at FROM tbl")
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		for rows.Next() {
			var dest time.Time
			if err := rows.Scan(&dest); err != nil {
				log.Println(err)
			}
			fmt.Println("phase 1", dest)
		}
		releaseProcess1()

		blockProcess2()
		rows, err = tx.Query("SELECT created_at FROM tbl")
		if err != nil {
			tx.Rollback()
			log.Println(err)
			return
		}
		for rows.Next() {
			var dest time.Time
			if err := rows.Scan(&dest); err != nil {
				log.Println(err)
			}
			fmt.Println("phase 2", dest)
		}

		tx.Commit()
	}()

	wg.Wait()

}
```

**REPEATABLE READ**

REPEATABLE READ solves the problems that READ UNCOMMITTED allows. It guarantees that any rows a transaction reads will "look the same" in subsequent reads within the same transaction, but in theory it still allows another tricky problem: phantom reads. Simply put, a phantom read can happen when you select some range or rows, another transaction inserts a new row into the range, and then you select the same range again; you will see the new "phantom" row. InnoDB and XtraDB solve the phantom read problem with multi-version concurrency control, which we ex-plain later in this chapter.

REPEATABLE READ is MySQL's default transaction isolation level.

**SERIALIZATION**

The highest level of isolation level, SERIALIZABLE, solves the phantom read problem by forcing transactions to be ordered so that they can't possibly conflict. In a nutshell, SERIALIZABLE places a lock on every row it reads. At this level, a lot of timeouts and lock contention can occur. We've rarely seen people use this isolation level, but your application's needs might force you to accept the decreased concurrency in favor of the data stability that result.

ANSI SQL isolation levels

|Isolation level|Drity reads possible|Nonrepeatable reads possible|Phantom reads possible|Locking reads|
|-|-|-|-|-|
|READ UNCOMMITTED|Yes|Yes|Yes|No|
|READ COMMITED|No|Yes|Yes|No|
|REPEATABLE READ|No|No|Yes|No|
|SERIALIZABLE|No|No|No|Yes|

## Deadlocks

A deadlock is when two or more transactions are mutually holding and requesting locks on the same resources, creating a cycle of dependencies. Deadlocks occur when transactions try to lock resources in a different order. They can happen whenever multiple transactions lock the same resources. For example, consider these two transactions running against the StockPrice table:

Transaction #1
    START TRANSACTION;
    UPDATE StockPrice SET close = 45.50 WHERE stock_id = 4 and date = '2002-05-01'; // lock row 1
    UPDATE StockPrice SET close = 19.80 WHERE stock_id = 3 and date = '2002-05-02'; // lock row 2
    COMMIT;

Transaction #2
    START TRANSACTION;
    UPDATE StockPrice SET high = 20.12 WHERE stock_id = 3 and date = '2002-05-02'; // lock row 2
    UPDATE StockPrice SET high = 47.20 WHERE stock_id = 4 and date = '2002-05-01'; // lock row 1
    COMMIT;

If you're unlucky, each transaction will execute its first query and update a row of data, locking it in the process. Each transaction will then attempt to update its second row, only to find that it is already locked. The two transactions will wait forever for each other to complete, unless something intervenes to break the deadlock.

To combat this problem, database systems implement various forms of deadlock detection and timeouts. The more sophisticated systems, such as the InnoDB storage engine, will notice circular dependencies and return an error instantly. This can be a good thing - otherwise, deadlocks would manifest themselves as very slow queries. Other will give up after the query exceeds a lock wait timeout, which is not always good. The way InnoDB currently handles deadlocks is to roll back the transaction that has the fewest exclusive row locks.

Deadlocks cannot be broken without rolling back one of the transactions, either partially or wholly. They are a fact of life in transaction systems, and your application should be designed to handle them. Many applications can simply retry their transactions from the beginning.

## Transaction Logging

Transaction logging helps make transactions more efficient. Instead of updating the tables on disk each time a change occurs, the storage engine can change its in-memory copy of the data. This is very fast. The storage engine can then write a record of the change to the transaction log, which is on disk and therefore durable. This is also a relatively fast operation, because appending log events involves sequential I/O in one small area of the disk instead of random I/O in many places. Then, at some later time, a process can update the table on disk. Thus, most storage engines that use this technique (known as write-ahead logging) end up writing the changes to disk twice.

If there's a crash after the update is written to the transaction log but before the changes a made to the data itself, the storage engine call still recover the changes upon restart. The recovery method varies between storage engines.

## Transactions in MySQL

MySQL provides two transactional storage engines: InnoDB and NDB Cluster. Several third-party engines are also available; the best-known engines right now are XtraDB and PBXT.

### AUTOCOMMIT

MySQL operates in AUTOCOMMIT mode by default. This means that unless you've explicitly begun a transaction, it automatically executes each query in a separate transaction. You can enable or disable AUTOCOMMIT for the current connection by setting a variable:

```sql
SHOW VARIABLES LIKE 'AUTOCOMMIT';

+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| autocommit    | ON    |
+---------------+-------+
```

### Mixing storage engines in transactions

MySQL doesn't manage transactions at the server level. Instead, the underlying storage engines implement transactions themselves. This means you can't reliably mix different engines in a single transaction.

If you mix transactional and non-transactional tables (for instance, InnoDB and MyISAM tables) in a transaction, the transaction will work properly if all goes well. However, if a rollback is required, the changes to the non-transactional table can't undoes.

### Implicit and explicit locking

InnoDB uses a two-phase locking protocol. It can require locks at any time during a transaction, but it does not release them util a COMMIT or ROLLBACK. It releases all the clocks at the same time. InnoDB handles locks automatically, according to your isolation level.

However, InnoDB also supports explicit locking, which the SQL standard does not mention at all:

- SELECT ... LOCK IN SHARE MODE
- SELECT ... FOR UPDATE

MySQL also supports the LOCK TABLES and UNLOCK TABLES commands, which are implemented in the server, not in the storage engines. These have their uses but they are not a substitute for transactions. If you need transactions, use a transactional storage engine.

We often see applications that have been converted from MyIsAM to InnoDB but are still using LOCK TABLES. This is no longer necessary because of row-level locking, and it can cause severe performance problems.

